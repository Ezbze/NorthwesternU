Is the killing and dismemberment of a journalist a mistake? How about the collision of an experimental car with a woman walking her bike across the road? During an interview aired on Sunday, Uber CEO Dara Khosrowshahi said he believed both were mistakes. The interview, filmed with journalists Mike Allen and Dan Primack for the program Axios on HBO, touched on the company’s relationship to its fifth largest shareholder, Saudi Arabia's sovereign wealth fund. (The governor of the fund, Yasir al-Rumayyan, also sits on Uber’s board.)“That government said that they made a mistake,” Khosrowshahi said of the murder of Jamal Khashoggi, in a response to a question about Uber’s continued relationship with Saudi Arabia. (Last year, The Washington Post columnist was last seen entering the Saudi Arabian consulate in Istanbul, Turkey; he never came out. A few months later, the CIA concluded that Saudi Crown Prince Mohammed bin Salman himself had ordered the journalist’s assassination.)Then Khosrowshahi likened the murder to mistakes Uber has made. “We've made mistakes too, with self-driving, and we stopped driving and we're recovering from that mistake. So I think that people make mistakes. It doesn't mean that they can never be forgiven. I think they've taken it seriously.” The self-driving “mistake” Khosrowshahi alluded to was the March 2018 collision between a testing self-driving car and an Arizona woman, who, according to recently released documents, was thrown 75 feet by the crash and died from her injuries.Khosrowshahi soon realized that he had made a mistake, and 12 hours after the interview aired, the CEO took to Twitter to backtrack. “There's no forgiving or forgetting what happened to Jamal Khashoggi & I was wrong to call it a ‘mistake,’” he wrote. “As I told @danprimack after our interview, I said something in the moment I don't believe. Our investors have long known my views here & I'm sorry I wasn’t as clear on Axios.” (Axios says Khosrowshahi did call to transmit a similar message the day after he sat for the interview.)The Uber CEO is far from the first tech exec to get cagey about his relationship with Saudi Arabia in the months after the Khashoggi killing. Just last week, two bits of related news surfaced—that the country had allegedly paid Twitter employees to spy for them; and that former Uber chief and enfant terrible Travis Kalanick had reportedly attempted, unsuccessfully, to obscure that the sovereign wealth fund was a significant source of funding for his new startup. Hey, there’s a lot of money in blinders: Mohammed bin Salman said recently that the fund was on track to managing $600 billion in assets in 2020.Dismissing Uber’s own self-driving errors as mere “mistakes” feels wrong too (although on a different order of magnitude). Especially given the raft of documents released last week by the federal transportation safety watchdog the National Transportation Safety Board, which has spent the last 20 months investigating the context of the accident, in which a car killed the woman, named Elaine Herzberg. During Sunday’s interview, Primack asked whether the crash boiled down to a “bad sensor.” “Yes, yeah,” Khosrowshahi responded, before Primack cut him off. But according to the documents, that’s not quite true. In fact, a series of poor decisions appear to have led to that moment on a dark Arizona road. (In May, an Arizona prosecutor said there was “no basis for criminal liability for the Uber corporation arising from” the fatal crash. On November 19, the NTSB will announce the final results of its investigation, saying who and what it believe is at fault for the crash.)According to the NTSB investigation, Uber’s software was not created to recognize pedestrians outside of crosswalks. “The system design did not include a consideration for jaywalking pedestrians,” one of the documents said. As a result, Uber’s system wasted some 4.4 seconds trying to “classify” Herzberg, and to use that information to predict her movement.Then, with just 1.2 seconds until impact, the Uber system again did what it was designed to do: It held off braking for one second. This aspect of the system was meant to give the “mission specialist” hired to monitor the self-driving car from behind the wheel time to verify “the nature of the detected hazard” and take action. According to the NTSB documents, Uber created “action suppression” system because the self-driving program’s developmental software kept having false alarms—that is, identifying hazards on the roads where none existed—and so kept executing unnecessary but “extreme” maneuvers, like swerving or hard braking. But on that night in March, the woman behind the wheel of the car didn’t look up during that second-long period, and the system only began to slow down 0.2 seconds before impact. In the end, the car was traveling at 43.5 mph when it hit Herzberg.And if the self-driving system had flaws, maybe those can be traced to a series of decisions Uber made around its organizational structure. The NTSB documents note that, while Uber’s self-driving unit did have a system safety team, it didn’t have an operational safety division or a safety manager. Nor did it have a formal safety plan, or a standardized operating procedure or guiding document for safety—the stuff of a well-thought-out “safety culture.” In fact, the company had only recently decided to depart from industry standards and have just one single person in each testing vehicle instead of two. (“We deeply value the thoroughness of the NTSB’s investigation into the crash and look forward to reviewing their recommendations once issued after the NTSB’s board meeting later this month,” an Uber spokesperson said last week in a statement.)So, does Uber get to be forgiven? That’s probably for the company and its customers to decide. For part of Monday morning, #BoycottUber trended nationwide on Twitter. Uber says it has completely revamped its self-driving testing procedures since the crash, and has added another person to each of the vehicles it tests on public roads. To cut down on mistakes.