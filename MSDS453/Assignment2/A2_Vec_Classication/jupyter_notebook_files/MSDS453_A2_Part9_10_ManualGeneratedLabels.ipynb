{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Week 5: A.2 Second Research/Programming Assignment using manually generated classes/labels a.k.a y_train, y_pred\n",
    "\n",
    "#Due No Due Date Points 100 Submitting a file upload File Types zip, pdf, py, txt, ipynb, and html\n",
    "#This assignment concerns vectorization and document classification.\n",
    "\n",
    "#In this assignment, you can continue to work with your individual corpus or work with a corpus \n",
    "#that you identify from the course or available public-domain sources. The Reduced Reuters Corpus may not \n",
    "#be used for this assignment because extensive jump-start code is provided for that corpus. \n",
    "#The corpus should have between two and ten identified classes of documents so that document \n",
    "#classification can be performed as the final step of the study. \n",
    "#The class of a document could be defined by the document source, with a known external variable, \n",
    "#or with a variable that you, the analyst, define. It could be a subtopic within \n",
    "#the general topic of interest used to define the corpus.\n",
    "\n",
    "#Consider three methods for assigning numerical vectors to documents. \n",
    "#For each method, obtain a vector of numbers representing each document in the corpus. \n",
    "#Represent these as row vectors, creating a documents-by-terms matrix for each vectorization method. \n",
    "#We refer to the columns as \"terms,\" but, depending on the method being employed, \n",
    "#these could be individual words, n-grams, tokens, or (as is the case for Doc2Vec) index positions along a vector.\n",
    "\n",
    "#Approach 1: Analyst Judgment.\n",
    "#As we have reviewed in classroom discussions, initial work with document collections could begin \n",
    "#with identifying important terms or equivalence classes (ECs) to be included \n",
    "#in a corpus-wide Reference Term Vector (RTV). \n",
    "#One way to do this is to employ analyst judgment guided by corpus statistics. \n",
    "\n",
    "#To decide on whether or not we will keep a term in a small document collection, \n",
    "#for example, we need to know that: (1) It is important in at least one document, \n",
    "#and (2) It is prevalent in more than one document.\n",
    "\n",
    "#For larger document collections, we may specify percentages of documents \n",
    "#in which we observe the terms or ECs. Analyst judgment is critical to this approach.\n",
    "\n",
    "#After the important terms have been identified, we can assign a number (perhaps a count or proportion) \n",
    "#for each term in each document. That is, we can define a vector of numbers for each document.\n",
    "\n",
    "#Approach 2: TF-IDF.\n",
    "\n",
    "#Identify the top terms by corpus-wide statistics (TF-IDF, in particular). \n",
    "#Regarding TF-IDF, we can compute the TF-IDF for each extracted term across the entire corpus. \n",
    "#For our reference vector, we can choose a subset of terms with the highest TF-IDF values across the corpus. \n",
    "#A high TF-IDF means that the term is both prevalent (across the corpus) \n",
    "#and prominent (within at least one or more documents). Additionally, \n",
    "#we have the TF-IDF value for each term within each document. \n",
    "#Python Scikit Learn provides TF-IDF vectorization:\n",
    "\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html \n",
    "\n",
    "\n",
    "#Approach 3: Neural Network Embeddings (Doc2Vec).\n",
    "\n",
    "#With this approach, we utilize machine learning methods to convert documents to vectors of numbers. \n",
    "#Such methods draw on self-supervised machine learning (autoencoding a la Word2Vec). \n",
    "#Instead of Word2Vec, however, we use Doc2Vec, representing each document with a set of numbers. \n",
    "#The numbers come from neural network weights or embeddings. The numbers are not directly associated \n",
    "#with terms, so the meaning of the numbers is undefined. Python Gensim provides Doc2Vec vectorizations:\n",
    "\n",
    "#https://radimrehurek.com/gensim/models/doc2vec.html (Links to an external site.)\n",
    "\n",
    "#Management Problem. Part of your job in this assignment is to define a meaningful management problem. \n",
    "#The corpus you use should relate in some way to a business, organizational, \n",
    "#or societal problem. Regardless of the neural network methods being employed, \n",
    "#research results should provide guidance in addressing the management problem. \n",
    "\n",
    "#Research Methods/Programming Components\n",
    "\n",
    "#This research/programming assignment involves ten activities as follows:\n",
    "\n",
    "#(1) Define a management goal for your research. What do you hope to learn \n",
    "#    from this natural language study? What management questions will be addressed? \n",
    "#     Consider a goal related to document classification.\n",
    "#(2) Identify the individual corpus you will be using in the assignment. \n",
    "#     The corpus should be available as a JSON lines file. \n",
    "#     Previously, we had suggested that the JSON lines file be set up with at \n",
    "#     least four key-value pairs defined as \"ID,\" \"URL,\" \"TITLE,\", and \"BODY,\" \n",
    "#     where \"BODY\" represents a plain text document. To facilitate subsequent analyses, \n",
    "#     it may be convenient to use a short character string (say, eight characters or less) \n",
    "#     to identify each document. This short character string could be the value associated \n",
    "#     with the ID key or with an additional key that you define. \n",
    "#(3) Preprocess the text documents, ensuring that unnecessary tags, \n",
    "#    punctuation, and images are excluded.  \n",
    "#(4) Create document vectors using Approach 1 above.\n",
    "#(5) Create document vectors using Approach 2 above.\n",
    "#(6) Create document vectors using Approach 3 above.\n",
    "#(7) Compare results across the three approaches. \n",
    "#     In comparing Approach 1 with Approach 2, for example, \n",
    "#     find the two or three terms (nouns/noun phrases) \n",
    "#     from your documents that you thought to be important/prevalent \n",
    "#     from Approach 1 and see if they did indeed have the highest TF-IDF as shown \n",
    "#     in the results from Approach 2. Similarly, find two or three terms that \n",
    "#     you thought would have a lower importance/prevalence, and see if that bears out. \n",
    "#     Judge the degree of agreement across the approaches.\n",
    "#(8) Review results in light of the management goal for this research. \n",
    "#     Do you have concerns about the corpus? Are there ways that the corpus should be extended \n",
    "#     or contracted in order to address management questions?\n",
    "#(9) Prepare numerical matrices for further analysis. For each of the three vectorization approaches, \n",
    "#    construct a matrix of real numbers with rows representing documents \n",
    "#    (i.e. each row is a numerical vector representing a document). \n",
    "#    We could refer to each matrix as a documents-by-terms matrix \n",
    "#    (although the elements of vectors from Approach 3 are not directly associated with terms). \n",
    "#    To facilitate future research with these matrices, rows should be associated with short \n",
    "#    character strings representing the documents in the corpus. Also, for Approaches 1 and 2, \n",
    "#    it will be convenient to have columns identified by character strings for the terms. \n",
    "#    If the terms are n-grams (groups of n words in sequence), it may be a good idea to \n",
    "#    replace blank characters with underlines, so that the columns may be \n",
    "#    interpreted as variable names in modeling programs.\n",
    "#(10) Working with the two to ten classes of documents for this exercise, \n",
    "#    use a random forest classifier to fit three text classification models, \n",
    "#    one for each of the vectorization methods. Determine the vectorization \n",
    "#    method that does the best job of classification based on an index of classification accuracy. \n",
    "#    If the number of documents is large or if classes have not been identified in advance, \n",
    "#    select a subset of the documents under study (perhaps 100 or 200), \n",
    "#    identify each document with a class or category. An example of this type \n",
    "#    of analysis is shown in example jump-start code under\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import re,string\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "#Functionality to turn stemming on or off\n",
    "STEMMING = True  # judgment call, parsed documents more readable if False\n",
    "MAX_NGRAM_LENGTH = 1  # try 1 and 2 and see which yields better modeling results\n",
    "VECTOR_LENGTH = 100  # set vector length for TF-IDF and Doc2Vec\n",
    "DISPLAYMAX = 10 # Dispaly count for head() or tail() sorted values\n",
    "DROP_STOPWORDS = False\n",
    "SET_RANDOM = 9999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "### Utility Functions \n",
    "##############################################################################\n",
    "# define list of codes to be dropped from document\n",
    "# carriage-returns, line-feeds, tabs\n",
    "codelist = ['\\r', '\\n', '\\t']    \n",
    "\n",
    "# text parsing function for entire document string\n",
    "def parse_doc(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'&(.)+', \"\", text)  # no & references  \n",
    "    text = re.sub(r'pct', 'percent', text)  # replace pct abreviation  \n",
    "    text = re.sub(r\"[^\\w\\d'\\s]+\", '', text)  # no punct except single quote \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r'', text)  # no non-ASCII strings    \n",
    "    if text.isdigit(): text = \"\"  # omit words that are all digits    \n",
    "    for code in codelist:\n",
    "        text = re.sub(code, ' ', text)  # get rid of escape codes  \n",
    "    # replace multiple spacess with one space\n",
    "    text = re.sub('\\s+', ' ', text)        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text parsing for words within entire document string\n",
    "# splits the document string into words/tokens\n",
    "# parses the words and then recreates a document string\n",
    "# returns list of parsed words/tokens and parsed document string\n",
    "def parse_words(text): \n",
    "    # split document into individual words\n",
    "    tokens=text.split()\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out tokens that are one or two characters long\n",
    "    tokens = [word for word in tokens if len(word) > 2]\n",
    "    # filter out tokens that are more than twenty characters long\n",
    "    tokens = [word for word in tokens if len(word) < 21]\n",
    "    # filter out stop words if requested\n",
    "    if DROP_STOPWORDS:\n",
    "        tokens = [w for w in tokens if not w in stoplist]         \n",
    "    # perform word stemming if requested\n",
    "    if STEMMING:\n",
    "       lem = WordNetLemmatizer()\n",
    "       tokens = [lem.lemmatize(word) for word in tokens]\n",
    "    # recreate the document string from parsed words\n",
    "    text = ''\n",
    "    for token in tokens:\n",
    "        text = text + ' ' + token\n",
    "    return tokens, text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label(text):\n",
    "    #print(text)\n",
    "    text = text.replace('.html','')\n",
    "    #print(text)\n",
    "    text = text.replace('.htm','')\n",
    "    if 'wired-' in text:\n",
    "        text = text.replace('wired-','w-')\n",
    "    elif 'nhtsa-' in text:\n",
    "        text = text.replace('nhtsa-', 'n-')\n",
    "    elif 'curbed-' in text:\n",
    "        text = text.replace('curbed-','c-')\n",
    "    elif 'theverge-' in text:\n",
    "        text = text.replace('theverge-','v-')\n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    regex.sub('', text)\n",
    "    return text[0:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Analyst Judgement to manually identify the important terms (classes) \n",
    "#    and their equivalent classes \n",
    "#######################################################################\n",
    "def get_y_class(text):     \n",
    "    safety = len(re.findall('safety',text))\n",
    "    technology = len(re.findall('technology',text))\n",
    "    crash = len(re.findall('crash',text))\n",
    "    people = len(re.findall('people',text))\n",
    "    driver = len(re.findall('driver',text))\n",
    "    human= len(re.findall('human',text))\n",
    "    lidar= len(re.findall('lidar',text))\n",
    "    autonomous= len(re.findall('autonomous',text))\n",
    "    research = len(re.findall('research ',text))\n",
    "    selfdriving  = len(re.findall('selfdriving ',text))\n",
    "    policy  = len(re.findall('policy ',text))\n",
    "    standard  = len(re.findall('standard ',text))\n",
    "    rule  = len(re.findall('rule ',text))\n",
    "    government = len(re.findall('government ',text))\n",
    "    testing = len(re.findall('testing ',text))\n",
    "    electric  = len(re.findall('electric ',text))\n",
    "    engineer  = len(re.findall('engineer ',text))\n",
    "    system  = len(re.findall('system ',text))    \n",
    "    \n",
    "    # Safety     = safety, crash, people, driver, human and policy\n",
    "    # Technology = technology, libar, autonomous, research, selfdriving\n",
    "    safetyClassCount = safety + crash + people + driver + human + policy + standard + rule + government + testing\n",
    "    technologyClassCount = technology + lidar + autonomous + research + selfdriving + electric + engineer +  system\n",
    "    \n",
    "    # Safety class == 0, Technology class = 1\n",
    "    if safetyClassCount < technologyClassCount:\n",
    "       return 1\n",
    "   \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "text_body=[]\n",
    "text_titles = []\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "with open('autonomous_vehicles_safety_corpus.jl') as json_file:\n",
    "     data = json.load(json_file)\n",
    "     for p in data:\n",
    "         text_body.append(p['BODY'])\n",
    "         text_titles.append(p['TITLE'][0:8])\n",
    "         labels.append(create_label(p['FILENAME'][0:8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# generate split of train and test data and\n",
    "# random sample by that split data by k number\n",
    "#############################################################################\n",
    "def train_test_split_with_random_sampling(text_body, k):\n",
    "    # list of token lists for gensim Doc2Vec\n",
    "    train_tokens = []\n",
    "    # list of document strings for sklearn TF-IDF\n",
    "    train_text = [] \n",
    "    # list of y variables from class labels analyst judgement\n",
    "    train_target = [] \n",
    "    train = random.sample(text_body, k)\n",
    "    \n",
    "    for doc in train:\n",
    "        text_string = doc\n",
    "        # parse the entire document string\n",
    "        text_string = parse_doc(text_string)\n",
    "        # parse words one at a time in document string\n",
    "        tokens, text_string = parse_words(text_string)\n",
    "        train_tokens.append(tokens)\n",
    "        train_text.append(text_string)\n",
    "        train_target.append(get_y_class(text_string))\n",
    "                \n",
    "    return train_tokens, train_text, train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "679"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train =int(len(text_body)*0.8)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = int(len(text_body) - train)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "### Prepare Training and test Data random sampling 80% training sample\n",
    "##############################################################################\n",
    "train_k =int(len(text_body)*0.8)\n",
    "test_k = int(len(text_body) - train_k)\n",
    "train_tokens, train_text, train_target = train_test_split_with_random_sampling(text_body, train_k) \n",
    "test_tokens, test_text, test_target = train_test_split_with_random_sampling(text_body, test_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of processor cores: 16\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "#  Number of cpu cores\n",
    "##############################################################################\n",
    "cores = multiprocessing.cpu_count()\n",
    "print(\"\\nNumber of processor cores:\", cores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\tCount Vectorization. . .\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "### Count Vectorization\n",
    "##############################\n",
    "\n",
    "print('\\n\\t\\tCount Vectorization. . .')\n",
    "count_vectorizer = CountVectorizer(ngram_range = (1, MAX_NGRAM_LENGTH), \n",
    "    max_features = VECTOR_LENGTH)\n",
    "count_vectors = count_vectorizer.fit_transform(train_text)\n",
    "\n",
    "#print('\\nTraining count_vectors_training.shape:', count_vectors.shape)\n",
    "\n",
    "# Apply the same vectorizer to the test data\n",
    "# Notice how we use count_vectorizer.transform, \n",
    "#                      NOT count_vectorizer.fit_transform\n",
    "count_vectors_test = count_vectorizer.transform(test_text)\n",
    "#print('\\nTest count_vectors_test.shape:', count_vectors_test.shape)\n",
    "count_clf = RandomForestClassifier(n_estimators = 100, max_depth = 10, random_state = SET_RANDOM)\n",
    "count_clf.fit(count_vectors, train_target)\n",
    "\n",
    "# evaluate on test set\n",
    "count_pred = count_clf.predict(count_vectors_test)  \n",
    "\n",
    "cv_RF_F1 = round(metrics.f1_score(test_target, count_pred, average='macro'), 3)\n",
    "#print('\\nCount/Random forest F1 classification performance in test set:', cv_RF_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\tTFIDF vectorization. . .\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "### TF-IDF Vectorization\n",
    "##############################\n",
    "print('\\n\\t\\tTFIDF vectorization. . .')\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range = (1, MAX_NGRAM_LENGTH), \n",
    "    max_features = VECTOR_LENGTH)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(train_text)\n",
    "\n",
    "#print('\\nTraining tfidf_vectors_training.shape:', tfidf_vectors.shape)\n",
    "\n",
    "# Apply the same vectorizer to the test data\n",
    "# Notice how we use tfidf_vectorizer.transform, NOT \n",
    "# tfidf_vectorizer.fit_transform\n",
    "tfidf_vectors_test = tfidf_vectorizer.transform(test_text)\n",
    "#print('\\nTest tfidf_vectors_test.shape:', tfidf_vectors_test.shape)\n",
    "tfidf_clf = RandomForestClassifier(n_estimators = 100, max_depth = 10, random_state = SET_RANDOM)\n",
    "\n",
    "tfidf_clf.fit(tfidf_vectors, train_target)\n",
    "\n",
    "# evaluate on test set\n",
    "tfidf_pred = tfidf_clf.predict(tfidf_vectors_test) \n",
    "tfidf_RF_F1 = round(metrics.f1_score(test_target, tfidf_pred, average='macro'), 3) \n",
    "#print('\\nTF-IDF/Random forest F1 classification performance in test set:', tfidf_RF_F1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = [TaggedDocument(doc, [i]) for i, doc in enumerate(train_tokens)]\n",
    "#print('train_corpus[:2]:', train_corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\tDoc2Vec Vectorization (50 dimensions). . .\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "### Doc2Vec Vectorization (50 dimensions)\n",
    "###########################################\n",
    "print('\\n\\t\\tDoc2Vec Vectorization (50 dimensions). . .')\n",
    "\n",
    "#print(\"\\nWorking on Doc2Vec vectorization, dimension 50\")\n",
    "model_50 = Doc2Vec(vector_size = 50, window = 4, min_count = 2, workers = cores, epochs = 40)\n",
    "model_50.build_vocab(train_corpus)\n",
    "\n",
    "# build vectorization model on training set\n",
    "model_50.train(train_corpus, total_examples = model_50.corpus_count, \n",
    "\tepochs = model_50.epochs)  \n",
    "\n",
    "# vectorization for the training set\n",
    "# initialize numpy array\n",
    "doc2vec_50_vectors = np.zeros((len(train_corpus), 50)) \n",
    "\n",
    "for i in range(0, len(train_tokens)):\n",
    "    doc2vec_50_vectors[i,] = model_50.infer_vector(train_tokens[i]).transpose()   \n",
    "    \n",
    "#print('\\nTraining doc2vec_50_vectors.shape:', doc2vec_50_vectors.shape)\n",
    "#print('doc2vec_50_vectors[:2]:', doc2vec_50_vectors[:2])\n",
    "\n",
    "# vectorization for the test set\n",
    "# initialize numpy array\n",
    "doc2vec_50_vectors_test = np.zeros((len(test_tokens), 50)) \n",
    "\n",
    "for i in range(0, len(test_tokens)):\n",
    "    doc2vec_50_vectors_test[i,] = model_50.infer_vector(test_tokens[i]).transpose()\n",
    "\n",
    "#print('\\nTest doc2vec_50_vectors_test.shape:', doc2vec_50_vectors_test.shape)\n",
    "\n",
    "doc2vec_50_clf = RandomForestClassifier(n_estimators = 100, max_depth = 10, \n",
    "                                        random_state = SET_RANDOM)\n",
    "\n",
    "# fit model on training set\n",
    "doc2vec_50_clf.fit(doc2vec_50_vectors, train_target) \n",
    "\n",
    "# evaluate on test set\n",
    "doc2vec_50_pred = doc2vec_50_clf.predict(doc2vec_50_vectors_test) \n",
    "\n",
    "# print the F1 score\n",
    "doc2vec_50_RF_F1 = round(metrics.f1_score(test_target, doc2vec_50_pred, average='macro'), 3)\n",
    "#print('\\nDoc2Vec_50/Random forest F1 classification performance in test set:', doc2vec_50_RF_F1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\tDoc2Vec Vectorization (100 dimensions). . .\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "### Doc2Vec Vectorization (100 dimensions)\n",
    "###########################################\n",
    "print('\\n\\t\\tDoc2Vec Vectorization (100 dimensions). . .')\n",
    "\n",
    "model_100 = Doc2Vec(train_corpus, vector_size = 100, window = 4, \n",
    "\tmin_count = 2, workers = cores, epochs = 40)\n",
    "\n",
    "model_100.train(train_corpus, total_examples = model_100.corpus_count, \n",
    "\tepochs = model_100.epochs)  # build vectorization model on training set\n",
    "\n",
    "# vectorization for the training set\n",
    "# initialize numpy array\n",
    "doc2vec_100_vectors = np.zeros((len(train_tokens), 100)) \n",
    "\n",
    "for i in range(0, len(train_tokens)):\n",
    "    doc2vec_100_vectors[i,] = model_100.infer_vector(train_tokens[i]).transpose()\n",
    "    \n",
    "#print('\\nTraining doc2vec_100_vectors.shape:', doc2vec_100_vectors.shape)\n",
    "\n",
    "\n",
    "# vectorization for the test set\n",
    "# initialize numpy array\n",
    "doc2vec_100_vectors_test = np.zeros((len(test_tokens), 100))\n",
    "\n",
    "for i in range(0, len(test_tokens)):\n",
    "    doc2vec_100_vectors_test[i,] = model_100.infer_vector(test_tokens[i]).transpose()\n",
    "    \n",
    "#print('\\nTest doc2vec_100_vectors_test.shape:', doc2vec_100_vectors_test.shape)\n",
    "\n",
    "doc2vec_100_clf = RandomForestClassifier(n_estimators = 100, max_depth = 10, \n",
    "                                         random_state = SET_RANDOM)\n",
    "\n",
    "# fit model on training set\n",
    "doc2vec_100_clf.fit(doc2vec_100_vectors, train_target) \n",
    "\n",
    "# evaluate on test set\n",
    "doc2vec_100_pred = doc2vec_100_clf.predict(doc2vec_100_vectors_test)  \n",
    "\n",
    "# print the F1 score\n",
    "\n",
    "doc2vec_100_RF_F1 =  round(metrics.f1_score(test_target, doc2vec_100_pred, average='macro'), 3)\n",
    "#print('\\nDoc2Vec_100/Random forest F1 classification performance in test set:', doc2vec_100_RF_F1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\tDoc2Vec Vectorization (100 dimensions). . .\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "### Doc2Vec Vectorization (100 dimensions)\n",
    "###########################################\n",
    "print('\\n\\t\\tDoc2Vec Vectorization (100 dimensions). . .')\n",
    "\n",
    "model_100 = Doc2Vec(train_corpus, vector_size = 100, window = 4, min_count = 2, workers = cores, epochs = 40)\n",
    "\n",
    "model_100.train(train_corpus, total_examples = model_100.corpus_count, \n",
    "\tepochs = model_100.epochs)  # build vectorization model on training set\n",
    "\n",
    "# vectorization for the training set\n",
    "# initialize numpy array\n",
    "doc2vec_100_vectors = np.zeros((len(train_tokens), 100)) \n",
    "\n",
    "for i in range(0, len(train_tokens)):\n",
    "    doc2vec_100_vectors[i,] = model_100.infer_vector(train_tokens[i]).transpose()\n",
    "    \n",
    "#print('\\nTraining doc2vec_100_vectors.shape:', doc2vec_100_vectors.shape)\n",
    "\n",
    "\n",
    "# vectorization for the test set\n",
    "# initialize numpy array\n",
    "doc2vec_100_vectors_test = np.zeros((len(test_tokens), 100))\n",
    "\n",
    "for i in range(0, len(test_tokens)):\n",
    "    doc2vec_100_vectors_test[i,] = model_100.infer_vector(test_tokens[i]).transpose()\n",
    "    \n",
    "#print('\\nTest doc2vec_100_vectors_test.shape:', doc2vec_100_vectors_test.shape)\n",
    "\n",
    "doc2vec_100_clf = RandomForestClassifier(n_estimators = 100, max_depth = 10, random_state = SET_RANDOM)\n",
    "\n",
    "# fit model on training set\n",
    "doc2vec_100_clf.fit(doc2vec_100_vectors, train_target) \n",
    "\n",
    "# evaluate on test set\n",
    "doc2vec_100_pred = doc2vec_100_clf.predict(doc2vec_100_vectors_test)  \n",
    "\n",
    "# print the F1 score\n",
    "\n",
    "doc2vec_100_RF_F1 =  round(metrics.f1_score(test_target, doc2vec_100_pred, average='macro'), 3)\n",
    "#print('\\nDoc2Vec_100/Random forest F1 classification performance in test set:', doc2vec_100_RF_F1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\tDoc2Vec Vectorization (200 dimensions). . .\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "### Doc2Vec Vectorization (200 dimensions)\n",
    "###########################################\n",
    "print('\\n\\t\\tDoc2Vec Vectorization (200 dimensions). . .')\n",
    "\n",
    "model_200 = Doc2Vec(train_corpus, vector_size = 200, window = 4, \n",
    "\tmin_count = 2, workers = cores, epochs = 40)\n",
    "\n",
    "# build vectorization model on training set\n",
    "model_200.train(train_corpus, total_examples = model_200.corpus_count, \n",
    "\tepochs = model_200.epochs) \n",
    "\n",
    "# vectorization for the training set\n",
    "# initialize numpy array\n",
    "doc2vec_200_vectors = np.zeros((len(train_tokens), 200)) \n",
    "\n",
    "for i in range(0, len(train_tokens)):\n",
    "    doc2vec_200_vectors[i,] = model_200.infer_vector(train_tokens[i]).transpose()\n",
    "    \n",
    "#print('\\nTraining doc2vec_200_vectors.shape:', doc2vec_200_vectors.shape)\n",
    "# print('doc2vec_200_vectors[:2]:', doc2vec_200_vectors[:2])\n",
    "\n",
    "# vectorization for the test set\n",
    "# initialize numpy array\n",
    "doc2vec_200_vectors_test = np.zeros((len(test_tokens), 200)) \n",
    "\n",
    "for i in range(0, len(test_tokens)):\n",
    "    doc2vec_200_vectors_test[i,] = model_200.infer_vector(test_tokens[i]).transpose()\n",
    "    \n",
    "#print('\\nTest doc2vec_200_vectors_test.shape:', doc2vec_200_vectors_test.shape)\n",
    "\n",
    "doc2vec_200_clf = RandomForestClassifier(n_estimators = 100, max_depth = 10, \n",
    "                                         random_state = SET_RANDOM)\n",
    "\n",
    "# fit model on training set\n",
    "doc2vec_200_clf.fit(doc2vec_200_vectors, train_target) \n",
    "\n",
    "# evaluate on test set\n",
    "doc2vec_200_pred = doc2vec_200_clf.predict(doc2vec_200_vectors_test) \n",
    "\n",
    "# print the F1 score\n",
    "doc2vec_200_RF_F1 = round(metrics.f1_score(test_target, doc2vec_200_pred, average='macro'), 3)\n",
    "\n",
    "#print('\\nDoc2Vec_200/Random forest F1 classification performance in test set:', doc2vec_200_RF_F1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = [[tfidf_RF_F1],\n",
    "                   [cv_RF_F1],\n",
    "                   [doc2vec_50_RF_F1],\n",
    "                   [doc2vec_100_RF_F1],\n",
    "                   [doc2vec_200_RF_F1]],\n",
    "                    columns=['F1 classificaton performance in test set with manual labeling'],\n",
    "                     index=['TF-IDF/Random forest classification',\n",
    "                            'CountVec/Random forest classification',\n",
    "                            'Doc2Vec_50/Random forest classification',\n",
    "                            'Doc2Vec_100/Random forest classification',\n",
    "                            'Doc2Vec_200/Random forest classification'])\n",
    "df.index.name ='Algorithm'\n",
    "df = df.sort_values('F1 classificaton performance in test set with manual labeling', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 classificaton performance in test set with manual labeling</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algorithm</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TF-IDF/Random forest classification</th>\n",
       "      <td>0.981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CountVec/Random forest classification</th>\n",
       "      <td>0.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2Vec_50/Random forest classification</th>\n",
       "      <td>0.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2Vec_100/Random forest classification</th>\n",
       "      <td>0.943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2Vec_200/Random forest classification</th>\n",
       "      <td>0.943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          F1 classificaton performance in test set with manual labeling\n",
       "Algorithm                                                                                              \n",
       "TF-IDF/Random forest classification                                                   0.981            \n",
       "CountVec/Random forest classification                                                 0.956            \n",
       "Doc2Vec_50/Random forest classification                                               0.956            \n",
       "Doc2Vec_100/Random forest classification                                              0.943            \n",
       "Doc2Vec_200/Random forest classification                                              0.943            "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
