It’s lunchtime, and the worker bees of Mountain View who aren’t interested in their company’s own catering are walking down East Middlefield Road in search of grub. It’s a lovely esplanade, but all the trees and light poles make these pedestrians hard to spot. That makes things tricky for a human driver, and extra troublesome for a robot trying to learn to work the wheel. But on a large monitor inside Soroush Salehian and Mina Rezk’s Mercedes Sprinter van, every meandering biped stands out against a sea of white. Those walking toward the van are blue, those moving away from it are red.The readout comes from the lidar laser sensor sitting in a box on the vehicle’s roof. Salehian and Rezk, cofounders of a startup called Aeva, have built one of the few sensors for self-driving cars that can see not just where things are, but how fast they’re moving. Moreover, inside that rooftop box is not just the lidar, but a camera. Aeva compiles the data streams into a single image, correlated down to the pixel and the nanosecond.1That’s a potentially crucial ability for cars trying to navigate a tricky world, which explains why VC firms Lux and Canaan have put $45 million into the startup. And where most sensor companies focus on one technology—lidar or cameras—Aeva is tackling everything and packing it into a single box any self-driving developer can use to show its vehicle the world.Salehian and Rezk founded Aeva early last year after working together on sensing technology at Apple’s highly secretive Special Projects Group. At Apple, Salehian also worked on the Apple Watch and Touch ID; Rezk came to that team from Nikon. Aeva (the name is a nod to the robot from Pixar’s Wall-E) now has about 50 employees, nearly all of them engineers, scooped from the likes of Apple, Tesla, and BMW.They have spent the past year and a half designing and building a new sort of Renaissance sensor whose most interesting component is the lidar. Nearly every one of the 50-plus companies in the field makes what’s called a time-of-flight system: It shoots out a million or more pulses of light a second and measures how long they take to bounce back after hitting the nearest object. The returns are compiled into a point cloud, which looks like a 3-D map of the sensor’s surroundings. The two difficult bits are looking at that point cloud and picking out what’s what and tracking it as it moves. To distinguish a child from a parking meter, you need good resolution—lots of laser pulses hitting that object. Following it through space and time requires hitting it again and again, so you can track a cluster of dots from one “frame” to another and deduce its movement.Aeva fires one steady beam of light instead of millions of discrete pulses, which allows it to measure the frequency of the returning photons. That’s key, because when light hits a moving object, its frequency changes just a bit—increasing if that object is approaching the sensor, decreasing if it’s moving away from it. Remember the doppler effect from high school physics, how a police siren sounds different coming toward you than it does going away from you? This is the same idea, with light instead of sound.“We’re focused on delivering things now,” says Aeva cofounder Mina Rezk. “This is an architecture that we put together, that we know we can manufacture.” That means no exotic materials and using components that are well established and easy to acquire.So Aeva’s lidar sensor doesn’t just create a 3-D map of the world, it flags everything that’s moving—and the stuff that’s moving tends to be extra important. Distinguishing the kid from the parking meter becomes a snap. Even better, you only need your laser to detect her a single time to know how fast she’s moving, and in which direction. “Within one frame, we know the velocity, where it’s heading, and where it’s going to be,” Salehian says.Velocity-tracking lidar isn’t new. Researchers have been using similar tech for decades to measure things like wind velocity. And Montana-based Blackmore is also applying the idea to self-driving car sensors. But Salehian and Rezk believe they’ve got a major advantage from putting it together with the camera that self-driving cars also use to see (cameras offer resolution no lidar can match) and save customers the headache of syncing it all up.It’s not clear, however, just how much value that really adds. “The ease of integration, I don’t really buy,” says Kevin Peterson, a cofounder and software lead for Marble, a startup working on sidewalk delivery robots. “We do it, and we’ve got 35 people. It’s not an impossible thing.”Along with measuring velocity, Aeva’s other highlights, Peterson says, seem more valuable, and come from that single beam of light. The whole setup uses under 100 watts—power consumption is a little-discussed but potentially serious pain point for cars decked out in sensors and computers. It also solves the looming problem of interference. In a world full of self-driving cars, it’s crucial to know that the photons you’re registering were shot out by your own lidar, not by that of another car on the street. Aeva can also use its system to determine the exact velocity of the car it’s sitting on. Most robocars now do that with an inertial measurement unit, Peterson says, but redundancy is always good.The really big question, Peterson says, is whether Aeva can deliver a lidar that is affordable and reliable, which is what matters if these vehicles are really going to enter commercial service in serious numbers. Here, Rezk is adamant. “We’re focused on delivering things now,” he says. “This is an architecture that we put together, that we know we can manufacture.” That means no exotic materials and using components that are well established and easy to acquire.“We are shipping our product to some of our automotive partners,” Salehian says, declining to specify which car companies he’s referring to. “This technology is available today.”Just in time, perhaps, for the robocar revolution.1Story updated at 18:00 ET on Monday October 1 to correctly list the contents of Aeva's rooftop sensor kit.