{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Week 5: A.2 Second Research/Programming Assignment  using k-means generated labels\n",
    "\n",
    "#Due No Due Date Points 100 Submitting a file upload File Types zip, pdf, py, txt, ipynb, and html\n",
    "#This assignment concerns vectorization and document classification.\n",
    "\n",
    "#In this assignment, you can continue to work with your individual corpus or work with a corpus \n",
    "#that you identify from the course or available public-domain sources. The Reduced Reuters Corpus may not \n",
    "#be used for this assignment because extensive jump-start code is provided for that corpus. \n",
    "#The corpus should have between two and ten identified classes of documents so that document \n",
    "#classification can be performed as the final step of the study. \n",
    "#The class of a document could be defined by the document source, with a known external variable, \n",
    "#or with a variable that you, the analyst, define. It could be a subtopic within \n",
    "#the general topic of interest used to define the corpus.\n",
    "\n",
    "#Consider three methods for assigning numerical vectors to documents. \n",
    "#For each method, obtain a vector of numbers representing each document in the corpus. \n",
    "#Represent these as row vectors, creating a documents-by-terms matrix for each vectorization method. \n",
    "#We refer to the columns as \"terms,\" but, depending on the method being employed, \n",
    "#these could be individual words, n-grams, tokens, or (as is the case for Doc2Vec) index positions along a vector.\n",
    "\n",
    "#Approach 1: Analyst Judgment.\n",
    "#As we have reviewed in classroom discussions, initial work with document collections could begin \n",
    "#with identifying important terms or equivalence classes (ECs) to be included \n",
    "#in a corpus-wide Reference Term Vector (RTV). \n",
    "#One way to do this is to employ analyst judgment guided by corpus statistics. \n",
    "\n",
    "#To decide on whether or not we will keep a term in a small document collection, \n",
    "#for example, we need to know that: (1) It is important in at least one document, \n",
    "#and (2) It is prevalent in more than one document.\n",
    "\n",
    "#For larger document collections, we may specify percentages of documents \n",
    "#in which we observe the terms or ECs. Analyst judgment is critical to this approach.\n",
    "\n",
    "#After the important terms have been identified, we can assign a number (perhaps a count or proportion) \n",
    "#for each term in each document. That is, we can define a vector of numbers for each document.\n",
    "\n",
    "#Approach 2: TF-IDF.\n",
    "\n",
    "#Identify the top terms by corpus-wide statistics (TF-IDF, in particular). \n",
    "#Regarding TF-IDF, we can compute the TF-IDF for each extracted term across the entire corpus. \n",
    "#For our reference vector, we can choose a subset of terms with the highest TF-IDF values across the corpus. \n",
    "#A high TF-IDF means that the term is both prevalent (across the corpus) \n",
    "#and prominent (within at least one or more documents). Additionally, \n",
    "#we have the TF-IDF value for each term within each document. \n",
    "#Python Scikit Learn provides TF-IDF vectorization:\n",
    "\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html \n",
    "\n",
    "\n",
    "#Approach 3: Neural Network Embeddings (Doc2Vec).\n",
    "\n",
    "#With this approach, we utilize machine learning methods to convert documents to vectors of numbers. \n",
    "#Such methods draw on self-supervised machine learning (autoencoding a la Word2Vec). \n",
    "#Instead of Word2Vec, however, we use Doc2Vec, representing each document with a set of numbers. \n",
    "#The numbers come from neural network weights or embeddings. The numbers are not directly associated \n",
    "#with terms, so the meaning of the numbers is undefined. Python Gensim provides Doc2Vec vectorizations:\n",
    "\n",
    "#https://radimrehurek.com/gensim/models/doc2vec.html (Links to an external site.)\n",
    "\n",
    "#Management Problem. Part of your job in this assignment is to define a meaningful management problem. \n",
    "#The corpus you use should relate in some way to a business, organizational, \n",
    "#or societal problem. Regardless of the neural network methods being employed, \n",
    "#research results should provide guidance in addressing the management problem. \n",
    "\n",
    "#Research Methods/Programming Components\n",
    "\n",
    "#This research/programming assignment involves ten activities as follows:\n",
    "\n",
    "#(1) Define a management goal for your research. What do you hope to learn \n",
    "#    from this natural language study? What management questions will be addressed? \n",
    "#     Consider a goal related to document classification.\n",
    "#(2) Identify the individual corpus you will be using in the assignment. \n",
    "#     The corpus should be available as a JSON lines file. \n",
    "#     Previously, we had suggested that the JSON lines file be set up with at \n",
    "#     least four key-value pairs defined as \"ID,\" \"URL,\" \"TITLE,\", and \"BODY,\" \n",
    "#     where \"BODY\" represents a plain text document. To facilitate subsequent analyses, \n",
    "#     it may be convenient to use a short character string (say, eight characters or less) \n",
    "#     to identify each document. This short character string could be the value associated \n",
    "#     with the ID key or with an additional key that you define. \n",
    "#(3) Preprocess the text documents, ensuring that unnecessary tags, \n",
    "#    punctuation, and images are excluded.  \n",
    "#(4) Create document vectors using Approach 1 above.\n",
    "#(5) Create document vectors using Approach 2 above.\n",
    "#(6) Create document vectors using Approach 3 above.\n",
    "#(7) Compare results across the three approaches. \n",
    "#     In comparing Approach 1 with Approach 2, for example, \n",
    "#     find the two or three terms (nouns/noun phrases) \n",
    "#     from your documents that you thought to be important/prevalent \n",
    "#     from Approach 1 and see if they did indeed have the highest TF-IDF as shown \n",
    "#     in the results from Approach 2. Similarly, find two or three terms that \n",
    "#     you thought would have a lower importance/prevalence, and see if that bears out. \n",
    "#     Judge the degree of agreement across the approaches.\n",
    "#(8) Review results in light of the management goal for this research. \n",
    "#     Do you have concerns about the corpus? Are there ways that the corpus should be extended \n",
    "#     or contracted in order to address management questions?\n",
    "#(9) Prepare numerical matrices for further analysis. For each of the three vectorization approaches, \n",
    "#    construct a matrix of real numbers with rows representing documents \n",
    "#    (i.e. each row is a numerical vector representing a document). \n",
    "#    We could refer to each matrix as a documents-by-terms matrix \n",
    "#    (although the elements of vectors from Approach 3 are not directly associated with terms). \n",
    "#    To facilitate future research with these matrices, rows should be associated with short \n",
    "#    character strings representing the documents in the corpus. Also, for Approaches 1 and 2, \n",
    "#    it will be convenient to have columns identified by character strings for the terms. \n",
    "#    If the terms are n-grams (groups of n words in sequence), it may be a good idea to \n",
    "#    replace blank characters with underlines, so that the columns may be \n",
    "#    interpreted as variable names in modeling programs.\n",
    "#(10) Working with the two to ten classes of documents for this exercise, \n",
    "#    use a random forest classifier to fit three text classification models, \n",
    "#    one for each of the vectorization methods. Determine the vectorization \n",
    "#    method that does the best job of classification based on an index of classification accuracy. \n",
    "#    If the number of documents is large or if classes have not been identified in advance, \n",
    "#    select a subset of the documents under study (perhaps 100 or 200), \n",
    "#    identify each document with a class or category. An example of this type \n",
    "#    of analysis is shown in example jump-start code under\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import re,string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#Functionality to turn stemming on or off\n",
    "STEMMING = True  # judgment call, parsed documents more readable if False\n",
    "MAX_NGRAM_LENGTH = 1  # try 1 and 2 and see which yields better modeling results\n",
    "VECTOR_LENGTH = 100  # set vector length for TF-IDF and Doc2Vec\n",
    "DISPLAYMAX = 10 # Dispaly count for head() or tail() sorted values\n",
    "DROP_STOPWORDS = False\n",
    "SET_RANDOM = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of processor cores: 16\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "#  Number of cpu cores\n",
    "##############################################################################\n",
    "cores = multiprocessing.cpu_count()\n",
    "print(\"\\nNumber of processor cores:\", cores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#  Create Labels \n",
    "############################################################################\n",
    "def create_label(text):\n",
    "    #print(text)\n",
    "    text = text.replace('.html','')\n",
    "    #print(text)\n",
    "    text = text.replace('.htm','')\n",
    "    if 'wired-' in text:\n",
    "        text = text.replace('wired-','w-')\n",
    "    elif 'nhtsa-' in text:\n",
    "        text = text.replace('nhtsa-', 'n-')\n",
    "    elif 'curbed-' in text:\n",
    "        text = text.replace('curbed-','c-')\n",
    "    elif 'theverge-' in text:\n",
    "        text = text.replace('theverge-','v-')\n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    regex.sub('', text)\n",
    "    return text[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "### Function to process documents\n",
    "###############################################################################\n",
    "def clean_doc(doc): \n",
    "    # split document into individual words\n",
    "    tokens=doc.split()\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 4]\n",
    "    # #lowercase all words\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]         \n",
    "    # # word stemming Commented\n",
    "    if STEMMING:   \n",
    "        lem = WordNetLemmatizer()\n",
    "        tokens = [lem.lemmatize(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "text_body=[]\n",
    "text_titles = []\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "with open('autonomous_vehicles_safety_corpus.jl') as json_file:\n",
    "     data = json.load(json_file)\n",
    "     for p in data:\n",
    "         text_body.append(p['BODY'])\n",
    "         text_titles.append(p['TITLE'][0:8])\n",
    "         labels.append(create_label(p['FILENAME']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# K Means to get the labels\n",
    "############################\n",
    "#empty list to store processed documents\n",
    "processed_text=[]\n",
    "#for loop to process the text to the processed_text list\n",
    "for i in text_body:\n",
    "    text=clean_doc(i)\n",
    "    processed_text.append(text)\n",
    "    \n",
    "#stitch back together individual words to reform body of text\n",
    "final_processed_text=[]\n",
    "\n",
    "for i in processed_text:\n",
    "    temp_DSI=i[0]\n",
    "    for k in range(1,len(i)):\n",
    "        temp_DSI=temp_DSI+' '+i[k]\n",
    "    final_processed_text.append(temp_DSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Vectorization\n",
      "\n",
      "\t\tTF-IDF Vectorization. . .\n",
      "\n",
      "TF-IDF Vectorization K Means vectorization. . .\n",
      "(679, 195)\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "### TF-IDF Vectorization\n",
    "##############################\n",
    "# run tfidf (prevalent - require 25% of docs)\n",
    "\n",
    "print('\\nTF-IDF Vectorization')\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,1), min_df=0.25)\n",
    "tfidf_matrix = tfidf.fit_transform(final_processed_text)\n",
    "\n",
    "print('\\n\\t\\tTF-IDF Vectorization. . .')\n",
    "#print('\\nTraining tfidf_matrix.shape:', tfidf_matrix.shape)\n",
    "\n",
    "print('\\nTF-IDF Vectorization K Means vectorization. . .')\n",
    "k=3\n",
    "km = KMeans(n_clusters=k, random_state=89)\n",
    "km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()\n",
    "#print(clusters)\n",
    "\n",
    "y = clusters\n",
    "X = tfidf_matrix\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=23)\n",
    "print( x_train.shape)\n",
    "\n",
    "\n",
    "# Apply the same vectorizer to the test data\n",
    "# Notice how we use tfidf_vectorizer.transform, NOT tfidf_vectorizer.fit_transform\n",
    "tfidf_clf = RandomForestClassifier(n_estimators = 100, max_depth = 10, \n",
    "\trandom_state = SET_RANDOM)\n",
    "\n",
    "tfidf_clf.fit(x_train, y_train)\n",
    "tfidf_pred = tfidf_clf.predict(x_val)  # evaluate on test set\n",
    "\n",
    "tfidf_RF_F1 = round(metrics.f1_score(y_val, tfidf_pred, average='macro'), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\tCount Vectorization\n",
      "\n",
      "Count Vectorization K Means vectorization. . .\n"
     ]
    }
   ],
   "source": [
    "#####################################################################################################\n",
    "### Count Vectorization\n",
    "#####################################################################################################\n",
    "print('\\n\\t\\tCount Vectorization')\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range = (1, MAX_NGRAM_LENGTH), max_features = VECTOR_LENGTH)\n",
    "count_vectors_matrix = count_vectorizer.fit_transform(final_processed_text)\n",
    "\n",
    "print('\\nCount Vectorization K Means vectorization. . .')\n",
    "\n",
    "kmCV = KMeans(n_clusters=k, random_state=89)\n",
    "kmCV.fit(count_vectors_matrix)\n",
    "clustersCv = kmCV.labels_.tolist()\n",
    "\n",
    "y1 = clustersCv\n",
    "X1 = count_vectors_matrix\n",
    "\n",
    "x_trainC, x_valC, y_trainC, y_valC = train_test_split(X1, y1, test_size=0.2, random_state=23)\n",
    "#print( x_train.shape)\n",
    "\n",
    "count_clf = RandomForestClassifier(n_estimators = 100, max_depth = 10, random_state = SET_RANDOM)\n",
    "count_clf.fit(x_trainC, y_trainC)\n",
    "count_pred = count_clf.predict(x_valC)  # evaluate on test set\n",
    "\n",
    "cv_RF_F1 =  round(metrics.f1_score(y_valC, count_pred, average='macro'), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "# train_corpus using TaggedDocument\n",
    "##################################################################################################\n",
    "train_corpus = [TaggedDocument(doc, [i]) for i, doc in enumerate(processed_text)]\n",
    "#print('train_corpus[:2]:', train_corpus[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\tWorking on Doc2Vec vectorization, dimension 50\n",
      "\n",
      "Doc2Vec 50 Vectorization K Means vectorization. . .\n",
      "(679, 195)\n"
     ]
    }
   ],
   "source": [
    "#################################################################################################\n",
    "### Doc2Vec Vectorization (50 dimensions)\n",
    "#################################################################################################\n",
    "\n",
    "print(\"\\n\\t\\tWorking on Doc2Vec vectorization, dimension 50\")\n",
    "\n",
    "model_50 = Doc2Vec(vector_size = 50, window = 4, min_count = 2, workers = cores, epochs = 40)\n",
    "model_50.build_vocab(train_corpus)\n",
    "model_50.train(train_corpus, total_examples = model_50.corpus_count, \n",
    "\tepochs = model_50.epochs)  # build vectorization model on training set\n",
    "\n",
    "# vectorization for the training set\n",
    "doc2vec_50_vectors = np.zeros((len(train_corpus), 50)) # initialize numpy array\n",
    "\n",
    "for i in range(0, len(processed_text)):\n",
    "    doc2vec_50_vectors[i,] = model_50.infer_vector(processed_text[i]).transpose()\n",
    "      \n",
    "#print('\\nTraining doc2vec_50_vectors.shape:', doc2vec_50_vectors.shape)\n",
    "\n",
    "print('\\nDoc2Vec 50 Vectorization K Means vectorization. . .')\n",
    "kmDV = KMeans(n_clusters=k, random_state=89)\n",
    "kmDV.fit(doc2vec_50_vectors)\n",
    "clustersDV = kmDV.labels_.tolist()\n",
    "\n",
    "y1 = clustersCv\n",
    "X1 = doc2vec_50_vectors\n",
    "\n",
    "x_traind, x_vald, y_traind, y_vald = train_test_split(X1, y1, test_size=0.2, random_state=SET_RANDOM)\n",
    "print( x_train.shape)\n",
    "\n",
    "doc2vec_50_clf = RandomForestClassifier(n_estimators = 100, max_depth = 10, random_state = SET_RANDOM)\n",
    "# fit model on training set\n",
    "doc2vec_50_clf.fit(x_traind, y_traind) \n",
    "\n",
    "# evaluate on test set\n",
    "doc2vec_50_pred = doc2vec_50_clf.predict(x_vald)  \n",
    "doc2vec_50_RF_F1 = round(metrics.f1_score(y_vald, doc2vec_50_pred, average='macro'), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\tWorking on Doc2Vec vectorization, dimension 100\n",
      "\n",
      "Doc2Vec 100 Vectorization K Means vectorization. . .\n",
      "(679, 195)\n"
     ]
    }
   ],
   "source": [
    "#################################################################################################\n",
    "### Doc2Vec Vectorization (100 dimensions)\n",
    "#################################################################################################\n",
    "\n",
    "print(\"\\n\\t\\tWorking on Doc2Vec vectorization, dimension 100\")\n",
    "\n",
    "model_100 = Doc2Vec(vector_size = 100, window = 4, \n",
    "\tmin_count = 2, workers = cores, epochs = 40)\n",
    "model_100.build_vocab(train_corpus)\n",
    "\n",
    "# build vectorization model on training set\n",
    "model_100.train(train_corpus, total_examples = model_100.corpus_count, epochs = model_100.epochs)  \n",
    "\n",
    "# vectorization for the training set\n",
    "doc2vec_100_vectors = np.zeros((len(train_corpus), 100)) # initialize numpy array\n",
    "\n",
    "for i in range(0, len(processed_text)):\n",
    "    doc2vec_100_vectors[i,] = model_100.infer_vector(processed_text[i]).transpose()\n",
    "      \n",
    "\n",
    "print('\\nDoc2Vec 100 Vectorization K Means vectorization. . .')\n",
    "kmDV = KMeans(n_clusters=k, random_state=89)\n",
    "kmDV.fit(doc2vec_100_vectors)\n",
    "clustersDV = kmDV.labels_.tolist()\n",
    "\n",
    "y1 = clustersCv\n",
    "X1 = doc2vec_100_vectors\n",
    "\n",
    "x_traind, x_vald, y_traind, y_vald = train_test_split(X1, y1, test_size=0.2, random_state=SET_RANDOM)\n",
    "print( x_train.shape)\n",
    "\n",
    "doc2vec_100_clf = RandomForestClassifier(n_estimators = 100, max_depth = 10, random_state = SET_RANDOM)\n",
    "# fit model on training set\n",
    "doc2vec_100_clf.fit(x_traind, y_traind) \n",
    "\n",
    "# evaluate on test set\n",
    "doc2vec_100_pred = doc2vec_100_clf.predict(x_vald)  \n",
    "doc2vec_100_RF_F1 = round(metrics.f1_score(y_vald, doc2vec_100_pred, average='macro'), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\tWorking on Doc2Vec vectorization, dimension 200\n",
      "\n",
      "Doc2Vec 200 Vectorization K Means vectorization. . .\n",
      "(679, 195)\n"
     ]
    }
   ],
   "source": [
    "#################################################################################################\n",
    "### Doc2Vec Vectorization (200 dimensions)\n",
    "#################################################################################################\n",
    "\n",
    "print(\"\\n\\t\\tWorking on Doc2Vec vectorization, dimension 200\")\n",
    "\n",
    "model_200 = Doc2Vec(vector_size = 200, window = 4, min_count = 2, workers = cores, epochs = 40)\n",
    "model_200.build_vocab(train_corpus)\n",
    "\n",
    "# build vectorization model on training set\n",
    "model_200.train(train_corpus, total_examples = model_200.corpus_count, epochs = model_200.epochs)  \n",
    "\n",
    "# vectorization for the training set\n",
    "doc2vec_200_vectors = np.zeros((len(train_corpus), 200)) # initialize numpy array\n",
    "\n",
    "for i in range(0, len(processed_text)):\n",
    "    doc2vec_200_vectors[i,] = model_200.infer_vector(processed_text[i]).transpose()\n",
    "      \n",
    "\n",
    "print('\\nDoc2Vec 200 Vectorization K Means vectorization. . .')\n",
    "kmDV = KMeans(n_clusters=k, random_state=89)\n",
    "kmDV.fit(doc2vec_200_vectors)\n",
    "clustersDV = kmDV.labels_.tolist()\n",
    "\n",
    "y1 = clustersCv\n",
    "X1 = doc2vec_200_vectors\n",
    "\n",
    "x_traind, x_vald, y_traind, y_vald = train_test_split(X1, y1, test_size=0.2, random_state=SET_RANDOM)\n",
    "print( x_train.shape)\n",
    "\n",
    "doc2vec_200_clf = RandomForestClassifier(n_estimators = 100, max_depth = 10, random_state = SET_RANDOM)\n",
    "# fit model on training set\n",
    "doc2vec_200_clf.fit(x_traind, y_traind) \n",
    "\n",
    "# evaluate on test set\n",
    "doc2vec_200_pred = doc2vec_200_clf.predict(x_vald)  \n",
    "doc2vec_200_RF_F1 = round(metrics.f1_score(y_vald, doc2vec_200_pred, average='macro'), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 classificaton performance in test set with k-means labeling</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algorithm</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TF-IDF/Random forest classification</th>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2Vec_50/Random forest classification</th>\n",
       "      <td>0.994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2Vec_100/Random forest classification</th>\n",
       "      <td>0.994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2Vec_200/Random forest classification</th>\n",
       "      <td>0.994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CountVec/Random forest classification</th>\n",
       "      <td>0.949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          F1 classificaton performance in test set with k-means labeling\n",
       "Algorithm                                                                                               \n",
       "TF-IDF/Random forest classification                                                   1.000             \n",
       "Doc2Vec_50/Random forest classification                                               0.994             \n",
       "Doc2Vec_100/Random forest classification                                              0.994             \n",
       "Doc2Vec_200/Random forest classification                                              0.994             \n",
       "CountVec/Random forest classification                                                 0.949             "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######################################################################################################\n",
    "#  Output the results to a DataFrame\n",
    "#######################################################################################################\n",
    "df = pd.DataFrame(data = [[cv_RF_F1],\n",
    "                   [tfidf_RF_F1],\n",
    "                   [doc2vec_50_RF_F1],\n",
    "                   [doc2vec_100_RF_F1],\n",
    "                   [doc2vec_200_RF_F1]],\n",
    "                    columns=['F1 classificaton performance in test set with k-means labeling'],\n",
    "                     index=['TF-IDF/Random forest classification',\n",
    "                            'CountVec/Random forest classification',\n",
    "                            'Doc2Vec_50/Random forest classification',\n",
    "                            'Doc2Vec_100/Random forest classification',\n",
    "                            'Doc2Vec_200/Random forest classification'])\n",
    "df.index.name ='Algorithm'\n",
    "df = df.sort_values('F1 classificaton performance in test set with k-means labeling', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
