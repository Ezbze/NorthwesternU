{"ID": 0, "URL": "https://www.wired.com/story/news-rules-clear-way-self-driving-cars/", "TITLE": "New Rules Could Finally Clear the Way for Self-Driving Cars | WIRED", "FILENAME": "wi-news-rules-clea", "BODY": "Since an internet search company named\nGoogle began testing self-driving vehicles more than a decade ago, the still-in-development technology has faced a niggling issue: The federal vehicle safety rules that govern car design assume that a human sits behind the steering wheel of each vehicle. In fact, they assume each vehicle has a steering wheel. Now those rules may begin to change. Last week, the National Highway Traffic Safety Administration (NHTSA) for the first time proposed tweaks to its vehicle safety rules to help ease the way for the widespread use of self-driving cars.Today, more than 60 federal standards govern the design, construction, performance, and durability of US motor vehicles, down to door locks, windshields, and electrical wiring. The proposed rules would make small changes to the language of the current standards, but could have a big impact on getting street-legal self-driving cars into production. The proposal is the furthest the federal agency has gone in writing new design regulations specifically for autonomous vehicles.The adjusted standards would nix some rules related to rider safety for autonomous vehicles that carry goods like groceries, but no people. They would address the protections needed when steering wheels and steering columns have gone the way of the dodo. They would reexamine how airbags might work in a car newly configured to drive itself, and consider barring children from the front left seat of a vehicle, where the steering wheel traditionally lives.The rules would also clarify the definition of \u201cdriver,\u201d which the agency has now decided it will not change within the standards. Instead, it will clarify within each mention whether \u201cdriver\u201d refers to a fleshy human, or the advanced driver systems that might one day operate cars all on their own. This is a reversal for the safety agency, which said in 2016 that it would legally redefine \u201cdriver\u201d to treat the Google self-driving system one.Stay in the know with our Transportation newsletter. Sign up here!The tweaks would give self-driving vehicle developers some\u2014but not all\u2014of the changes they say they need to run fully autonomous vehicles in US cities. In a 2018 petition to NHTSA, General Motors said it would need exemptions from 16 federal standards to operate its \u201cZero-Emission Autonomous Vehicles,\u201d which the company envisions as driver-free taxis, on public roads. NHTSA\u2019s proposed changes touch six of those standards, making it easier, for example, for GM to build a car without a steering wheel. But the changes don\u2019t address some of the company\u2019s other requests, including doing away with the rules for dashboard controls, rearview mirrors, and upper headlight beams.Anyone interested in the self-driving design rules will have two months to comment on the proposals before they are finalized. NHTSA says in its notice that it is studying other changes to the safety standards, including adjustments that would allow all seats within a driverless vehicle to face each other, or to recline flat.In the meantime, NHTSA says it expects self-driving vehicle developers to continue to seek exemptions for vehicles that don\u2019t quite fit existing federal standards. In February, the agency granted its first exemption to the delivery robot company Nuro, allowing the company to build up to 5,000 vehicles in the next two years without side-view mirrors and windshields. NHTSA has yet to respond to the 2018 application from General Motors.NHTSA has been criticized by safety advocates for its industry-friendly, hands-off approach to regulating self-driving vehicles and their testing. In November, in a report on a fatal 2018 collision of a self-driving Uber, the National Transportation Safety Board called on NHTSA to more proactively monitor and regulate how self-driving cars are testing on public roads.But NHTSA says in its new document that it thinks its strategy threads the needle. \u201cThe agency believes the most prudent path forward is to remove unnecessary barriers to innovation while ensuring that occupants continue to receive the same protections afforded by existing regulations,\u201d it writes.The US Department of Transportation first released guidelines in 2016 to give direction to developers of self-driving technology, and has updated them three times since. The guidelines emphasize safety, tech neutrality, and regulatory consistency, and they encourage companies to submit information about their approaches to safety. But they didn\u2019t create any rules around testing self-driving cars. Today, states oversee testing and have taken wildly different approaches.California, for example, requires developers to obtain a license to test, and to annually submit detailed information on its testing activities, which is released to the public. (Many in the industry argue that the data collected by the state doesn\u2019t provide a clear or accurate picture of its activities.)Arizona, by contrast, only requires companies to submit information about how their vehicles will interact with law enforcement and emergency services, and to guarantee in writing that their vehicles are autonomous. After that, Arizona does not monitor the vehicles, though its governor reserves the right to revoke any company\u2019s ability to operate after something goes wrong. Governor Doug Ducey did just that in 2018, kicking Uber\u2019s self-driving operations out of the state after a testing vehicle struck and killed a woman on foot."}{"ID": 1, "URL": "https://www.wired.com/story/snow-ice-pose-vexing-obstacle-self-driving-cars/", "TITLE": "Snow and Ice Pose a Vexing Obstacle for Self-Driving Cars | WIRED", "FILENAME": "wi-snow-ice-pose-v", "BODY": "In late 2018, Krzysztof Czarnecki, a professor at Canada\u2019s University of Waterloo, built a self-driving car and trained it to navigate surrounding neighborhoods with an annotated driving data set from researchers in Germany.The vehicle worked well enough to begin with, recognizing Canadian cars and pedestrians just as well as German ones. But then Czarnecki took the autonomous car for a spin in heavy Ontarian snow. It quickly became a calamity on wheels, with the safety driver forced to grab the wheel repeatedly to avert disaster.The incident highlights a gap in the development of self-driving cars: maneuvering in bad weather. To address the problem, Czarnecki and Steven Waslander, a professor at the University of Toronto, compiled a data set of images from snowy and rainy Canadian roads. It includes footage of foggy camera views, blizzard conditions, and cars sliding around, captured over two winters. The individual frames are annotated so that a machine can interpret what the scene conveys. Autonomous driving systems typically use annotated images to inform algorithms that track a car's position and plan its route.The Canadian data should help researchers develop and test algorithms against challenging conditions. But the team also hopes it will prompt carmakers and startups to think more about bad-weather driving. \u201cIt\u2019s in the interest of everyone to think about this,\u201d Czarnecki says.The Canadian driving data set includes lane markings and vehicles that are covered with snow.A few companies, including Alphabet\u2019s Waymo and Argo, backed by Ford and Volkswagen, are testing self-driving cars in winter conditions. But as a whole, the industry is far more focused on demonstrating and deploying vehicles in fair-weather locations such as California, Arizona, Texas, and Florida.As even optimists about self-driving cars temper forecasts on their arrival, testing in sunnier climates is seen as a way to move the technology out of first gear. But the warm-weather bias could limit where autonomous vehicles can be deployed, or cause problems if it is rolled out in colder climates too quickly.\u201cIt\u2019s a very noticeable blind spot,\u201d says Alexandr Wang, CEO of Scale AI, which annotated Czarnecki\u2019s data and works with other autonomous-driving companies. \u201cDeploying autonomous vehicles in bad conditions is not really tackled, or really talked about.\u201dSelf-driving cars will need to identify obstacles even when their sensors are impaired by snow or rain.Inclement conditions are challenging for autonomous vehicles for several reasons. Snow and rain can obscure and confuse sensors, hide markings on the road, and make a car perform differently. Beyond this, bad weather represents a difficult test for artificial intelligence algorithms. Programs trained to pick out cars and pedestrians in bright sunshine will struggle to make sense of vehicles topped with piles of snow and people bundled up under layers of clothing.\u201cYour AI will be erratic,\u201d Czarnecki says of the typical self-driving car faced with snow. \u201cIt\u2019s going to see things that aren\u2019t there and also miss things.\u201dMatthew Johnson-Roberson, a professor at the University of Michigan who is developing a delivery robot optimized for difficult weather conditions, believes tackling bad weather may offer a way to gain a competitive edge. Troubling conditions are a major source of accidents, he says, so they arguably should be a priority.A busy intersection from the poor-weather data set.\u201cThe really big players are not focused on this,\u201d says Johnson-Roberson. \u201cThere\u2019s still a lot of work to be done on self-driving cars in general, but [driving in bad weather] is going to be a big differentiator, and also important to scaling this.\u201dWaymo declined to comment, but a spokesperson pointed out that the company\u2019s newest sensors and software are better suited to challenging weather. A spokesperson for Argo said initial deployments of the company\u2019s technology should be able to handle light rain, but \u201cfor heavier rains and snow, there still needs to be advancements in both hardware and software.\u201dWhen industry players decide to tackle bad weather, they\u2019ll gather lots of training data of their own. But in the meantime, Czarnecki\u2019s data should help the field advance.\u201cAdverse weather presents tremendous challenges for automated driving technology, and I applaud these researchers for releasing a challenging-weather data set,\u201d says John Leonard, a professor at MIT who is also affiliated with the Toyota Research Institute. \u201cPublicly available data sets can have a huge positive impact on research in the field.\u201d\u201cThe complexity of winter weather is going to take an incredible amount of work for automation technology to tackle,\u201d says Bryan Reimer, a research scientist at MIT specializing in autonomous driving. \u201cIce-weather conditions are incredibly difficult.\u201dAs for tackling the worst conditions on, say, Interstate 70 through the Rocky Mountains, where 24-hour snow crews are required, Reimer thinks self-driving cars won\u2019t be there for a while. \u201cThe only way you\u2019re going to drive that autonomously is to heat the road,\u201d he says."}{"ID": 2, "URL": "https://www.wired.com/story/california-self-driving-cars-log-most-miles/", "TITLE": "In California, Which Self-Driving Cars Log the Most Miles? | WIRED", "FILENAME": "wi-california-self", "BODY": "How many times in 2019 did human safety operators have to take control of\nself-driving vehicles being tested in California? With the release of new reports Wednesday by the state's Department of Motor Vehicles, we can begin to know. What's more, robot cars in autonomous mode drove almost 2.9 million miles on public California roads last year, up from 2.1 million miles in 2018.For various reasons, disengagements are a crummy tool for measuring progress toward safe, efficient autonomy, and for comparing momentum across companies. For one, the players don\u2019t always define \u201cdisengagement\u201d in the same way. Plus, lots of the big autonomous vehicle players\u2014Waymo, Uber, and Argo AI, among others\u2014do the bulk of their testing outside of California. Companies have more sophisticated internal metrics for measuring their own success\u2014you can read about those here. The reports also do not include driving on private test facilities and campuses, nor the many more \u201cvirtual miles\u201d that autonomous vehicle systems travel in digital space.Want the latest news on self-driving cars in your inbox? Sign up here!But the reports aren\u2019t devoid of interesting information. They include how many miles each company covered and how many cars it operates, and the reason for each disengagement. Taken together and compared with data from years past, those data reveal a few noteworthy trends and details about the ongoing race to develop the self-driving car.Of the 36 companies that reported testing in California between December 1, 2018 and November 30, 2019, Cruise\u2019s filing may reveal the most about its progress. That\u2019s because the General Motors subsidiary does nearly all its public road testing in San Francisco, where it\u2019s aiming to launch a robotaxi service in the next few years. CEO Dan Ammann last year delayed Cruise\u2019s initial plan to launch in 2019, and said the company was building out its fleet of test vehicles to log more miles. Now, we know what he meant: Cruise ran 226 vehicles in California in 2019, up from 125 in 2018. It logged 831,000 miles, up from 447,600.In the same stretch, Apple\u2019s secretive self-driving effort built up its fleet, but effectively stopped driving on public roads. All of its disengagements occurred between June and November, indicating that its cars drove very little in the first half of the year. The company laid off more than 200 of its Project Titan employees in January 2019, then acquired (and hired employees from) the self-driving startup Drive.ai in June.Among other players, startup Zoox nearly doubled its mileage compared with 2018, as it seeks to test its custom-built vehicle this year, ahead of a planned robotaxi business, the Verge reports. Waymo\u2019s impressive 1,454,100 miles last year\u2014the equivalent of almost 2,000 round trips between San Francisco and Los Angeles\u2014represented a 15 percent increase from 2018. Nuro, which makes low-speed vehicles for food delivery, also piled on the distance. Its purpose-built vehicles are running pilots in Texas and Arizona; in California, Nuro tests with conventional cars retrofitted with self-driving smarts, with safety operators inside.All told, safety operators across California took control from their robots about 9,000 times, but a few companies accounted for most of those: Lyft, Mercedes-Benz, and Toyota each reported more than 1,600 disengagements. State law requires carmakers to explain each disengagement. The result is a sea of vagaries, euphemisms, and jargon, including \u201cPlanning Module Failure due to software issue;\u201d \u201ca software fault due to a potential performance issue with a software component of the self-driving system (including third-party software components);\u201d and \u201cincorrectly perceived trajectory of a nearby object led to a planned trajectory that overlaps with a nearby cyclist.\u201d But in the vastness lie a few interesting oases that help explain why these companies have spent tens of billions of dollars and more than a decade on this technology, and remain far from done.Cruise blames 43 of its 68 disengagements at least in part on other people \u201cbehaving poorly.\u201d That may refer to things that are technically illegal but socially acceptable, or at least predictable, such as a jaywalker or a quick three-point turn. Similarly (if more charitably), Zoox blames many of its disengagements on its system\u2019s failure to accurately predict where cyclists, pedestrians, scooters, and other vehicles are headed. Nuro drivers had to take over because their robot had trouble with palm fronds, garbage bags, and animals in the road. To recast Melisandre, the roads may be well lit, but are full of terrors. That means next year\u2019s batch of disengagement reports may show progress, but don\u2019t expect a finished product."}{"ID": 3, "URL": "https://www.wired.com/story/how-self-driving-car-makers-measure-progress/", "TITLE": "How Self-Driving Car Makers Measure Their Own Progress | WIRED", "FILENAME": "wi-how-self-drivin", "BODY": "It\u2019s report card season for\nself-driving cars. On Wednesday, the California Department of Motor Vehicles released reports detailing how much the companies permitted to test autonomous vehicles in the state drove last year, and how often their human safety operators had to take control from the computer. The \u201cdisengagement reports\u201d provide a rare glimpse into the workings of companies developing robots on public streets.But it\u2019s too bad the reports are nearly useless for gauging how close we are to the age of autonomy. First off, the companies use different jargon to explain various disengagements. They only cover California, while most of the big players do the majority of their testing elsewhere\u2014Waymo around Phoenix, Argo in Pittsburgh and Miami, and Aptiv in Las Vegas, to name a few.Want the latest news on self-driving cars in your inbox? Sign up here!More fundamentally, disengagements are a poor way to measure progress. They\u2019re not good for comparing companies, because rivals test in different places (Cruise in complex San Francisco, Waymo in calmer suburbs, and so on). The companies also follow different protocols: Some tell their drivers to take control in school zones or when emergency vehicles are nearby, generating disengagements in spots where the vehicle might have done just fine. Perhaps most damning is that the best way to limit disengagements\u2014racking up miles in easy, well-studied areas\u2014is a bad way to improve an autonomous system. Waymo said Wednesday the reports don't \"provide relevant insights\" into its self-driving program \"or distinguish its performance from others in the self-driving space.\"So how do the companies track their progress? Some metrics are straightforward. If your vision system is only detecting 98 percent of pedestrians, your machine-learning algorithm probably needs to study more examples, in the hope of getting beyond 99.99 percent. At least once a month, Matt Johnson-Roberson, CEO of Refraction AI, goes over such stats, along with things like how often the computers crash and how reliably Refraction\u2019s vehicles follow their software\u2019s instructions. Refraction is building a small robot that sticks to the bike lane, making food deliveries in Ann Arbor, Michigan.While the startup and its competitors have their particular ways of measuring progress, most appear to focus less on how many miles they\u2019ve driven than on the range of situations they can navigate safely.First step: Consider what the vehicle will have to do. The go-anywhere, anytime robocar is likely decades away; most developers are targeting a niche constrained by geography, road type, and driving conditions. Cruise\u2019s cars will have to handle all of San Francisco, which effectively means they have to be able to do anything a human can\u2014unprotected left turns, four-way stops, roundabouts, the crazy steep streets that made the Bullitt car chase so fun. Optimus Ride and Voyage are going after retirement communities and other circumscribed areas, which require fewer capabilities.You make a list of those capabilities, something like a syllabus, that you need to teach the car. The companies testing today started with fundamentals like writing the code that tells a car to pick out and stay between lane lines. Then you might add changing lanes, merging onto a highway, or slowing for another driver cutting into your lane. Any time you change the software controlling the car, you first try it in computer simulation, to see how it works and identify bugs. Then you typically put it into a vehicle for testing on a private track in controlled conditions. Once it\u2019s proven there, you can move onto public roads. Waymo, for example, has driven 20 million miles in the real world\u2014and more than 10 billion in the virtual one.As each function improves, \u201cyou can start crossing them off the list,\u201d says Don Burnette, who runs the self-driving truck outfit Kodiak Robotics. \u201cHow many features do you still have left to implement? How many features have you included? That is a very good indicator of progress for a company\u201d\u2014one that Kodiak uses internally.At the same time, you make each feature more capable. If you\u2019re working on lane changing, you start with no other vehicles around, focusing on a human-like trajectory and speed. (Again, this work happens first in simulation, then in the real world.) Then you add a few cars to the scene, then more cars, so yours has to decide when it\u2019s safe to move into smaller and smaller gaps. Eventually, you work on creating a gap, the way a human driver nudges another to let him in. It\u2019s the same way you teach a person a new thing, say how to speak French: Begin with \u201ccombien co\u00fbte une madeleine,\u201d and work your way up to reading Proust.Once you\u2019ve crossed everything off your list of capabilities, you have a \u201cfeature complete\u201d system. The height of that bar\u2014an environment like a major city calls for a nearly endless list of skills\u2014helps explain why so many self-driving outfits are pursuing more limited business models like truckling and shuttle vans. Unsurprisingly, the ever-confident Elon Musk is the rare person to claim victory. \u201cI think we will be \u2018feature-complete\u2019 on full self-driving this year,\u201d Musk said in early 2019. \u201cMeaning the car will be able to find you in a parking lot, pick you up, take you all the way to your destination without an intervention this year.\u201d In an earnings call last month, he explained that \u201cfeature complete just means it has some chance of going from your home to work with no interventions.\"Still, the gulf between \u201cfeature complete\u201d and \u201cmission accomplished\u201d is wide. Take Smart Summon, which Tesla released in September to autonomously guide a car from a parking spot to where its owner is standing. Anecdotal evidence says that mostly it works\u2014except for when the car confuses asphalt and grass, freezes, or pins itself against a garage door.So once you\u2019ve added a feature to your code base, you have to ensure it works in as many situations as possible. That\u2019s where simulation is crucial, says Chris Urmson, who led Waymo in its early years and is now CEO of Aurora, which is developing self-driving tech for a variety of applications, including trucking. Last year, when Urmson\u2019s team was working on unprotected turns, they first sent out human drivers on fact-finding missions. They were interested in sampling the variety of life: how quickly or slowly human drivers moved through different kinds of intersections, how badly a truck might block a car\u2019s view of oncoming traffic, and so on. They loaded the results into their simulation software, then made variations by \u201cfuzzing\u201d the details\u2014making slight changes to other actors\u2019 positions, speed, and so on. Before trying any actual left turns into traffic, Urmson says, Aurora ran more than 2 million experiments in simulation, continually honing how its system hanged louies.Then they took their robots to the streets to validate their computer learnings in the real world. Aurora\u2019s safety operators noted unusual situations and moments where the vehicle didn\u2019t behave the way they\u2019d have liked, which typically led to disengaging the autonomous system. Rather than focusing on the number of times they retook control, Aurora\u2019s engineers used those moments as fodder for more simulation, more fuzzing, and more tweaks that improve the car\u2019s skills.At some point, Urmson and his team will decide their system has flashed its skills in enough scenarios to enter the world without a human behind the wheel. Different developers will pull that trigger at different points, because nobody can agree on the much-fretted-over question: How safe is safe enough? That includes regulators. The federal Department of Transportation has offered only vague guidelines for developing safe systems. Many states have welcomed AV developers without imposing any technical requirements. California stands out: More than 60 companies are permitted to test their tech in the state, but just five have secured permission from the Public Utility Commission to carry passengers.Don\u2019t expect that light-handed arrangement to change, says Bryant Walker Smith, a professor at the University of South Carolina School of Law who studies automated vehicle policy. These vehicles run complex software in a complex environment. Regulators and the public won\u2019t have the expertise, resources, or time to fully understand how this all works, he adds. No company is likely to drive the number of miles it would take to offer statistical proof its creation is as capable (or more so) as a human. Which means everybody will have to take a leap of faith, or at least a hop, Walker Smith says. \u201cIt\u2019s up to the company developing and deploying that technology to be worthy of our trust.\u201dRefraction AI\u2019s robots are unlikely to hurt anyone too badly, since they move between 10 and 12 mph. So the team can look past safety to another metric: the cost of each delivery. Recently, engineers spent about a month working on four-way stops. They got the robot to a point where it \u201cnever failed,\u201d Johnson-Roberson says, but only because it was so conservative, waiting seven or eight minutes to make its move. So they decided to avoid the problem altogether, sending the bot on another route or having a human remotely guide it. (Teleoperation is an under appreciated but vital tool for making any self-driving system work.) This works because Refraction\u2019s future doesn\u2019t hinge on mastering the tricky nature of the four-way stop. The only metric that matters is whether it gets University of Michigan students their burgers and fries before they get cold."}{"ID": 4, "URL": "https://www.wired.com/story/why-are-parking-lots-so-tricky-for-self-driving-cars/", "TITLE": "Why Are Parking Lots So Tricky for Self-Driving Cars? | WIRED", "FILENAME": "wi-why-are-parking", "BODY": "Tesla CEO Elon Musk has always argued that data collection is his car company\u2019s secret sauce, and he did it again on Wednesday, on a call with investors. Having a big fleet of Tesla vehicles on the road is nice, he said, \u201cbecause it allows us to collect these corner cases and learn from them.\u201d Every so often, Tesla will push out an over-the-air software update, suddenly giving its cars the power to, say, fart on command, or play Netflix movies over the center console screen, or change lanes by themselves on the highway. (Musk has said that a Tesla purchased today has all the hardware needs to become a totally self-driving car\u2014once, of course, the company pushes out the right software update.) Then it will suck back information about how that feature is working in the real world, and use it to perfect the product.Last month, Tesla pushed out a feature called \u201cSmart Summon,\u201d which allows drivers to beckon their vehicles straight to them using the Tesla app, Knight Rider\\\u2013style. (The feature is only supposed to be used on private parking lots or driveways, and then only when the driver has their eyes on the vehicle.) And though \u201cSmart Summon\u201d is still in beta, Musk said on Wednesday that it has now been used over 1 million times. That means the company has captured 1 million maneuvers' worth of data on weird parking lot happenings, and can use it to make its cars drive themselves around parking lots with more, well, confidence.Some improvement is needed: Videos posted on social media show one Tesla that can\u2019t tell the difference between grass and asphalt, and another that almost dinged another car, and yet another that actually did. (The US Department of Transportation told Reuters earlier this month that it is aware of the Smart Summon feature and is \u201cgathering information\u201d about its safety performance.)Still, lots of Tesla drivers are plenty pleased with the way Smart Summon has navigated their precious vehicles through parking lots, and according to experts, that\u2019s impressive. Turns out navigating a parking lot is one of the more difficult things Elon Musk\u2014or anyone, really\u2014could ask a self-driving vehicle to do. \u201cIn terms of solving parking lots where [the tech] does it perfectly, better than humans? Yeah, that\u2019s hard,\u201d says Matthew Johnson-Roberson, assistant professor of engineering at the University of Michigan and the cofounder of Refraction AI, a startup that is building autonomous delivery vehicles. No wonder those fender-bender videos are floating around the internet.In fact, parking lots are one of the most human places you could put a car that doesn\u2019t need a human to drive. Their rules are not always consistent, and drivers, moreover, don\u2019t always follow them. They\u2019re full of little people-to-people interactions: a wave to let the dad behind the stroller that that you\u2019re going to stop and let him cross; a nod to tell the other driver to inform him that you\u2019re waiting for this woman fiddling with her keys to finally pull out of her spot. These are very complicated things for computer systems to learn, even if they\u2019re trained on tons and tons of real-life parking lot data.If the parking lot is underground, the car also might lose a valuable source of data\u2014its access to GPS. In that case, it\u2019s up to the car to \u201clocalize,\u201d or figure out where it is relative to other cars and people and shopping carts and walls, based on its other built-in sensors. (Today\u2019s Teslas, for example, come with eight cameras and a forward-facing radar unit, all of which help the car \u201csee.\u201d)Francesco Borrelli, a professor who studies automotive control systems at UC Berkeley, says this \u201clocalization\u201d task is especially hard in a parking lot or structure. \u201cLocalizing yourself in tight spaces only with cameras might be hard,\u201d he says. \u201cThat is a fact. If you had laser scanning, it would be easier.\u201d Musk has argued that Tesla vehicles will one day be able to operate completely autonomously without expensive laser sensors, or lidar\u2014so the cars come without it.Parking lots are especially human compared to a highway, where, as Johnson-Roberson puts it, \u201cautomated driving is effectively solved.\u201d On a highway, all vehicles are moving in the same direction and at around the same speed (unless something has gone terribly wrong). Cars generally have a few lanes in which to maneuver, and thus some \u201cgrace space\u201d to screw up in before the car smashes into anything. And highways are uniform places, mostly:\nno pedestrians, no bicycles, ample warnings before you hit a construction zone.But parking lots? Chaos! Confusion!One thing would make self-driving in parking lots much easier, experts say: vehicle-to-vehicle communication. If cars could coordinate with each other, instead of having to learn the vagaries of human behavior, they\u2019d have a much easier time in parking lots. In fact, Borrelli isn\u2019t quite convinced that an autonomous vehicle will ever be able to perfectly navigate a parking lot based on machine learning alone. Cars might need to perform the robot equivalent of nods and waves to get through those labyrinths.Still, there\u2019s one clear reason why Tesla\u2019s decision to tackle parking lots makes a lot of sense: It\u2019s way less risky. \u201cSmart Summon\u201d can operate at a top speed of 5 or 6 mph, so a crash would be more fender-bender than demolition derby. \u201cYou\u2019re not going to kill anybody,\u201d says Borrelli.\n\u201cIf something goes wrong, you scratch the car.\u201d That, of course, is what autobody shops are for."}{"ID": 5, "URL": "https://www.wired.com/story/regulating-self-driving-cars-no-one/", "TITLE": "Who's Regulating Self-Driving Cars? Often, No One | WIRED", "FILENAME": "wi-regulating-self", "BODY": "A few hundred self-driving cars are undergoing testing on American roads today, using advanced technology to journey down highways, stop at red lights, and avoid pedestrians and cyclists\u2014except when they don\u2019t. More than 60 companies are registered to test in California alone, though just 28 tested on state roads last year.Exactly how many vehicles are testing, where they\u2019re doing it, and how those cars are performing is mostly anyone\u2019s guess. In many states, companies experimenting with autonomous vehicles don\u2019t have to specify, and the federal government doesn\u2019t keep track either. Yes, the tech is still very much under development, and industry reps and experts say it\u2019s way too early to create some kind of robot self-driver\u2019s license exam. But two meetings in Washington, DC, last week made clear that some believe regulators aren\u2019t properly overseeing the testing of this technology.Want the latest news on self-driving cars in your inbox? Sign up here!In the first, the National Transportation Safety Board met to conclude its investigation into a 2018 collision in which an Uber self-driving vehicle hit and killed an Arizona woman. Board members railed against a system that has left most of the oversight of self-driving vehicles to states, and made the few federal guidelines voluntary.\u201cIn my opinion, they\u2019ve put technology advancement here before saving lives,\u201d said NTSB member Jennifer Homendy, referring to the National Highway Traffic Safety Administration, which regulates motor vehicle safety. \u201cThere\u2019s no requirement. There\u2019s no evaluation. There\u2019s no real standards issued.\u201dNTSB investigator Ensar Becic said the federal government was \u201cperfectly positioned\u201d to demand a more rigorous safety approach from self-driving vehicle companies. It could, for example, provide feedback on the voluntary safety self-assessments that the agency has politely asked companies to write and hand over. It could also make them mandatory.The next day, during a meeting of the Senate Commerce Committee, senators said they wanted to hasten the development of autonomous vehicles, which the industry says will drive more carefully than their human counterparts. But some Senate Democrats wondered, sometimes angrily, whether the federal government was doing enough to monitor the testing of self-driving cars right now.\u201cWhile I appreciate the potential benefits of autonomous vehicles, I remain concerned that humans will be used as test dummies,\u201d said Senator Tom Udall, the Democrat from New Mexico.Last December, the Senate\u2019s first bit of legislation that would have governed self-driving cars, a bill called AV START, sputtered and died after an analogous bill passed the House. The stakes are still high: More than 36,000 people died on American roads last year, according to federal statistics, including a growing number of pedestrians and cyclists. Slowing the development of self-driving vehicles, which should one day drive with more precision and way less distraction, might mean more lives lost in the long run. But some have begun to question whether autonomous vehicles, when they arrive, will be unmitigated saviors. And more deaths in the testing phase could decelerate their rollout too.Technically, regulating the testing of self-driving vehicles falls to the states. The federal government deals with stuff related to vehicle design; for example, it can recall a car if something is wrong with the airbag. At this stage, all the self-driving vehicles testing on public roads are just normal cars with some extra software thrown in. So they don\u2019t need special exemptions from the federal government to operate. (If companies like General Motors follow through with their plans to mass-produce cars without steering wheels, they will.)The states, however, typically deal with stuff related to how to operate vehicles. For example, they determine if you\u2019re a good enough driver to earn a license. And especially at the testing phase, before these cars are carrying around too many members of the public, state legislatures and agencies are in charge of determining who gets to do what.Today, state rules for self-driving vary wildly. Arizona, for instance, is a hotbed for self-driving testing in part because its governor in 2015 wrote an executive order telling state agencies to support their testing. Companies testing cars there with a backup driver behind the steering wheel\u2014as Uber did before it was kicked out following the 2018 crash\u2014must only notify the state that they\u2019re there. A company like Waymo, which has reportedly started carrying some paying passengers in driverless vehicles around the Phoenix suburbs, must fill out more forms to get permission to nix the backup driver. In a statement, a spokesperson for the state\u2019s Department of Transportation called Arizona\u2019s rules a \u201crigorous process,\u201d but said the department was reviewing the process after the release of the NTSB\u2019s report.Pennsylvania and California, by contrast, took years to develop their tougher self-driving rules. Potential Pennsylvania testers have to complete a more detailed application, which must be approved by the state, and requires companies testing AVs to tell the state where they\u2019re testing and report crashes. California collects and publishes information about testing vehicles\u2019 performance, much to companies\u2019 chagrin.Acting NHTSA administrator Jeremy Owens told senators at the hearing that the federal government, which has more expertise in software, often helps states write their regulations for self-driving. But some, like the watchdog group Consumer Reports, want more from the feds, like validating the safety of self-driving vehicles before they hit the road\u2014even if they\u2019re still just testing, and not being purchased by consumers.Companies can choose to submit detailed safety information to the federal government, as part of NHTSA\u2019s guidance on automated vehicles, which broadly outlines its approach on autonomous vehicles. But only 16 of the companies working on self-driving vehicles have chosen to do so, and the quality of the information is \u201ckind of all over the place,\u201d said Becic.In the end, the NTSB recommended that states like Arizona develop applications for testing self-driving vehicles on their roads, and force companies to disclose how they might manage the risk associated with operating the vehicles around real\u2014and sometimes unsuspecting\u2014people. The panel also called on NHTSA to force companies developing the tech to give it information on their approach to safety, and to develop standards for evaluating that information.NHTSA said in a statement that it \u201cwill carefully review\u201d the NTSB report and recommendations. It\u2019s possible we may be about to learn a lot more about what\u2019s rolling out on American roads."}{"ID": 6, "URL": "https://www.wired.com/story/teaching-self-driving-cars-watch-unpredictable-humans/", "TITLE": "Teaching Self-Driving Cars to Watch for Unpredictable Humans | WIRED", "FILENAME": "wi-teaching-self-d", "BODY": "If you happen to live in\none of the cities where companies are testing self-driving cars, you\u2019ve probably noticed that your new robot overlords can be occasional nervous drivers. In Arizona, where SUVs operated by Waymo are sometimes ferrying passengers without anyone behind the steering wheel, drivers have complained about the robot cars\u2019 too-timid left turns and slow merges on the highway. Data compiled by the state of California suggests that the most common self-driving fender benders are rear-end crashes, in part because human drivers don\u2019t expect autonomous cars to follow road rules and come to complete, non-rolling stops at stop signs.As for human drivers, some are nervous and scrupulous, others are definitely not. In fact, it\u2019s even more complex: Some drivers are careful in some moments and hard-charging in others. Think: casual Sunday drive to the grocery store versus racing to get the kid before the day care late fees kick in. Robot cars might be smoother, and might make better decisions, if they knew exactly what sort of humans were driving near them.Want the latest news on self-driving cars in your inbox? Sign up here!Researchers at MIT\u2019s Computer Science and Artificial Intelligence Laboratory and Delft University\u2019s Cognitive Robotics lab say they\u2019ve figured out how to teach self-driving vehicles just that. In a recent paper published in the Proceedings of the National Academy of Sciences, they describe a technique that translates sociology and psychology into a mathematical formula that can be used to teach self-driving software how to tell the road ragers from the rule followers. Vehicles equipped with their technique can differentiate between the two in about two seconds, the researchers say, and can use the info to help decide how to proceed on the road. The technique improves self-driving vehicles\u2019 predictions about human drivers\u2019 decisions, and therefore the vehicles\u2019 on-road performance, by 25 percent, as measured by a test involving merging in a computer simulation.The idea, the researchers say, is not just to create a system that can differentiate \u201cegoistic\u201d drivers from \u201cprosocial\u201d drivers\u2014that is, the selfish ones from generous ones. The scientists hope to make it easier for robots to adapt to human behavior, and not the other way around.\u201cWe are very much interested in how human-driven vehicles and robots can coexist,\u201d says Daniela Rus, director of the MIT lab and a coauthor of the paper. \u201cIt\u2019s a grand challenge for the field of autonomy and a question that\u2019s applicable not just for robots on roads but in general, for any kind of human-machine interaction.\u201d One day, this kind of work might be able to help humans work more smoothly with robots on, say, the factory floor or in a hospital room.But first, game theory. The research pulls from an approach being applied more frequently in robotics and machine learning: using games to \u201cteach\u201d machines to make decisions with imperfect knowledge. Game players\u2014like drivers\u2014often have to reach conclusions without full understanding of what the other players\u2014or drivers\u2014are doing. So more researchers are applying game theory to train self-driving cars how to act in uncertain situations.Still, the uncertainty is a challenge. \u201cUltimately, one of the challenges of self-driving is that you\u2019re trying to predict human behavior, and human behavior tends to not fall into rational agent models we have for game players,\u201d says Matthew Johnson-Roberson, assistant professor of engineering at the University of Michigan and the cofounder of Refraction AI, a startup building autonomous delivery vehicles. Someone might look like they\u2019re about to merge but see a flash of something out of the corner of their eye and stop short. It\u2019s very hard to teach a robot to predict that kind of behavior.Of course, driving situations could become less uncertain if the researchers were able to collect more information about human driving behavior, which is what they\u2019re hoping to do next. Data on the speed of vehicles, where they are heading, the angle at which they\u2019re traveling, how their position changes over time\u2014all could help traveling robots better understand how the human mind (and personality) operates. Perhaps, the researchers say, an algorithm derived from more precise data could improve predictions about human driving behavior by 50 percent instead of 25 percent.That might be really hard, says Johnson-Roberson. \u201cOne of the reasons I think it's going to be challenging to deploy [autonomous vehicles] is because you\u2019re going to have to get these predictions right when traveling at high speeds in dense urban areas,\u201d he says. Being able to tell whether a driver is a selfish driver within two seconds of observation is useful, but a car traveling at 25 mph travels nearly 75 feet in that time. A lot of unfortunate things can happen in 75 feet.The fact is, even humans don\u2019t understand humans all the time. \u201cPeople are just the way they are, and sometimes they\u2019re not focused on driving, and make decisions we can\u2019t completely explain,\u201d says Wilko Schwarting, an MIT graduate student who led the research. Good luck out there, robots."}{"ID": 7, "URL": "https://www.wired.com/story/gms-cruise-rolls-back-target-self-driving-cars/", "TITLE": "GM\u2019s Cruise Rolls Back Its Target for Self-Driving Cars | WIRED", "FILENAME": "wi-gms-cruise-roll", "BODY": "Cruise, the startup General Motors acquired to develop its\nself-driving car, will launch an autonomous taxi service on the gnarly, crowded streets of San Francisco, CEO Dan Ammann said Wednesday. It will not, however, do so by the end of this year, the deadline it set for itself in 2017. Instead, Cruise will spend the rest of 2019 expanding its tests across the city and working on the less technical aspects of running such a service, from charging its electric cars to working with regulators to soothing a public that may be wary of robots roaming the roads.The revised timeline underlines a reality that many industry prognosticators have resisted. Creating a self-driving car that can move through a city safely, reliably, and efficiently is a punishing task. Nobody has managed it. Nobody knows how much time, money, and manpower they\u2019ll need. And with a technology where mistakes can easily turn deadly, nobody wants to move ahead until they\u2019re confident they can do it without risking their reputation and financial well-being. When Waymo launched its service last December to meet its own deadline, it kept human backups behind the wheel\u2014an underwhelming result after a decade of work.\u201cAnytime that you\u2019re working on something that\u2019s never been done before, it\u2019s not surprising if timelines move around,\u201d Ammann says. \u201cIf we do it right from the outset, that\u2019s what will allow us to scale it up rapidly.\u201d At this point, that means delaying the outset to an unspecified time.Ammann took the Cruise CEO job at the end of last year, leaving his post as GM president to guide the outfit. (Cruise founder and former CEO Kyle Vogt is now the CTO.) Much of his work has been focused on stockpiling the cash he believes this effort will take. In the past year, Cruise has raised $7.25 billion, counting Softbank and Honda as major investors. It now has 1,500 employees\u2014nearly 40 people for each of the 40 employees it had at the time of GM\u2019s acquisition, in March 2016. It has built up a network of fast chargers around San Francisco to reenergize its cars\u2019 batteries, and plans to build more. And while the secrecy surrounding this nascent industry makes it hard to know who\u2019s leading the pack, those stats suggest that Cruise is one of the few players\u2014along with Waymo, Argo, Uber, and a handful of others\u2014positioned to deliver something as complex as a robo-taxi service.To this growing pile of money, people, and plugs, Ammann is adding miles. Cruise will significantly expand its tests in San Francisco. That means increasing (without offering specifics) its test fleet of sensor-clad Chevy Bolts, which currently numbers about 180, according to VentureBeat. It also will mean keeping the cars, and their human safety operators, on the streets more of the time. \u201cYou\u2019re going to see a lot more of them, doing a lot more miles,\u201d Ammann says. And while the AV industry has moved away from counting miles driven as a metric for progress, Ammann says that you can\u2019t drive in SF without encountering, and learning from, new situations. \u201cEvery mile is an adventure.\u201dCruise set the end of 2019 deadline for a commercial launch in November 2017 at an event for investors. Ammann, however, says he\u2019s not concerned about shifting the timeline. \u201cWe\u2019ll be gated by safety,\u201d he says. Meaning, better to get it right, right from the start.It\u2019s not surprising that Cruise hasn\u2019t specified a new deadline. Many startups are going after limited visions of autonomous vehicles, focusing on trucking, or short distance shuttles, or moving food through the bike lane. Cruise, like Waymo, remains focused on the marquee goal of a driverless taxi that anyone can summon on their smartphone and rely on for a ride. (Same goes for Ford, which is working with AV startup Argo and targeting 2021 for its service.) Cruise hasn\u2019t taken its sights off that golden ring\u2014it just can\u2019t tell you when it will make the grab."}{"ID": 8, "URL": "https://www.wired.com/story/safety-board-faults-tesla-regulators-fatal-2018-crash/", "TITLE": "A Safety Board Faults Tesla and Regulators in a Fatal 2018 Crash | WIRED", "FILENAME": "wi-safety-board-fa", "BODY": "A federal watchdog panel ripped into\nTesla and the federal agency that regulates it on Tuesday after concluding an investigation into a fatal 2018 crash involving the carmaker\u2019s driving assistance feature, Autopilot.The investigation by the National Transportation Safety Board found that Tesla hasn\u2019t done enough to prevent drivers\u2019 misuse of Autopilot, which Tesla says can \u201cdrive\u201d on highways in some situations but must always be monitored by those behind the wheel. The probe also concluded that the National Highway Traffic Safety Administration\u2019s hands-off, voluntary approach to regulating new vehicle technology had overlooked risks of advanced driver-assistance features, now found in vehicles made by Tesla, General Motors, Audi, and others.\u201cFixing problems after people die is not a good highway approach,\u201d Robert Malloy, the director of highway safety at the NTSB, told board members Tuesday.Want the latest news on self-driving cars in your inbox? Sign up here!The NTSB\u2019s investigation\u2014one of four completed or underway into Autopilot-related crashes\u2014examined a March 2018 incident in Mountain View, California. During the morning commute, a 2017 Model X on Autopilot slammed into a concrete barrier separating the highway from an exit at 71 mph. (The barrier had been damaged in another crash 11 days earlier and had not been repaired by California\u2019s highway agency.) The SUV spun, collided with two other vehicles, and then caught fire. Bystanders pulled the driver from the vehicle, but the man, a 38-year-old Apple engineer named Walter Huang, died soon after at a local hospital.The board determined that the probable cause of the crash was complicated\u2014but leveled some well-trod accusations against Tesla. It said that Tesla\u2019s Autopilot system steered the SUV off the road and that the driver failed to notice before the car slammed into the barrier seconds later. The NTSB concluded that Huang relied more heavily on the Autopilot feature than he should have; cell phone logs suggest he was playing a game on his phone. The safety board pinned some of that inattention on the design of Tesla Autopilot, which uses the amount of torque applied to the steering wheel as a sign that the driver is sufficiently engaged in driving. (Other systems, like the one used by General Motors, use an in-vehicle camera to monitor head position and eye movement.)The investigation found that Walter Huang did not have his hands on the steering wheel for six seconds before his Tesla Model X slammed into a highway divider.The NTSB can\u2019t change federal or local policy, but it can make recommendations. It had plenty of those. The board reiterated two recommendations it issued during a 2017 investigation of an Autopilot-related death\u2014to which Tesla has not formally responded. It asked that Tesla limit drivers to using Autopilot in road and weather situations where it can be used safely. And it urged the company to update its driver monitoring system so it can \u201cmore effectively sense\u201d how engaged the person behind the wheel is with driving.Tesla did not respond to a request for comment. But just a few days after the crash, the company released details about the incident\u2014a serious no-no in NTSB investigations, which typically take months or years to complete\u2014and effectively blamed Huang for the event. The company said Huang had not touched the wheel for six seconds before the vehicle slammed into the concrete lane divider and that \u201cthe driver had about five seconds and 150 meters of unobstructed view of the concrete divider \u2026 but the vehicle logs show that no action was taken.\u201d Because Tesla released that information, the NTSB took the unusual step of formally removing Tesla as party to the investigation.\u201cFixing problems after people die is not a good highway approach.\u201dRobert Malloy, director of highway safety, NTSBThe NTSB also said that California\u2019s highway agencies contributed to the incident by failing to fix the concrete barrier\u2014something it concluded would have saved the drivers\u2019 life.The NTSB had recommendations for the NHTSA, too. It urged the agency to come up with new ways to test advanced driver assistance features like forward collision warning, and to look into Tesla\u2019s Autopilot system in particular. The NTSB also asked the NHTSA to work with industry to create performance standards for driver assistance features, which critics say can lure humans into a sense of complacency even though they\u2019re not intended to take over the driving task.Board member Jennifer Homendy slammed federal regulators for their approach to new tech like Autopilot, which the NHTSA has championed as an effort to keep new vehicles reasonably priced and accessible to more drivers. \u201cNHTSA\u2019s mission isn't to sell cars,\u201d she said.In a statement, an NHTSA spokesperson said the agency would \u201ccarefully review\u201d the NTSB\u2019s report, the final version of which will be issued in coming weeks. It also pointed to agency research setting out industry best practices for driver assistance technology.By Alex Davies The NTSB also laid some blame for Huang\u2019s crash at the feet of his employer, Apple. Employers should have distracted-driving policies for their workers, the board said, prohibiting them from using devices while operating company-owned vehicles and from using their work devices while operating any vehicle. (Huang was operating his own vehicle at the time of the crash, but his phone was Apple-owned.) And the panel called on mobile device makers, including Apple, Google, Lenovo, and Samsung, to create tech that would lock drivers out of distracting applications while driving. The ask \u201cis not anti-technology,\u201d NTSB chair Robert Sumwalt said in his opening statement. \u201cIt is pro-safety.\u201d Apple said it expects its employees to follow the law.After a 2016 crash involving Autopilot, Tesla changed how the feature works. Now, if a driver using Autopilot doesn\u2019t put pressure on the wheel for 30 seconds, warnings will beep and flash until the car slows itself to a stop. Last year, the company updated Autopilot again with new warnings for red lights and stop signs.In response to a video that went viral last year appearing to show a driver sleeping while using Autopilot on a highway, US senator Ed Markey (D\u2013Massachusetts) recommended Tesla rename and rebrand Autopilot to make clear its limitations and add a back-up system to make sure the driver remains engaged while behind the wheel. In response to Markey, Tesla said it believes that drivers who misuse Autopilot are \u201ca very small percentage of our customer base\u201d and that many online videos of drivers misusing the feature \u201care fake and intended to capture media attention.\u201dBy comparing crash data from Tesla drivers with Autopilot engaged against data from those who only use more basic safety features like forward collision warning and automatic emergency braking, Tesla has concluded that it customers are 62 percent less likely to crash with Autopilot engaged.Still, NTSB wanted to emphasize one incontrovertible fact. \u201cYou do not own a self-driving car,\u201d Sumwalt said Tuesday. \u201cDon\u2019t pretend that you do.\u201d"}{"ID": 9, "URL": "https://www.wired.com/story/uber-self-driving-crash-volvo-polestar-1-roundup/", "TITLE": "The Failure of Uber's Self-Driving Car, Polestar's Debut, and More Car News This Week | WIRED", "FILENAME": "wi-uber-self-drivi", "BODY": "Mistakes do happen. But this week, when the National Transportation Safety Board\nreleased more than 40 documents related to the 2018 crash in which a self-driving Uber killed a pedestrian, it quickly became apparent that the term \u201cmistake\u201d might be generous. According to investigators, Uber\u2019s organizational structure wasn\u2019t built to catch safety errors, and its software included glaring safety holes. Like that the cars were explicitly programmed not to react to an oncoming crash for the space of 1 second, apparently because the software saw so many \u201cghosts\u201d that engineers seemed more worried about hard braking for no reason than because the vehicle was about to hit something. Or that the car wasn\u2019t \u201ctaught\u2019 to recognize pedestrians outside of crosswalks. It\u2019s ugly stuff.Happily, not all was awful in the transportation world this week. The federal government seems to be taking bicyclist safety seriously (though it has some serious work to do), and we got to take the slick, hybrid grand touring coupe Polestar 1 for a spin. It\u2019s been a week; let\u2019s get you caught up.Stories you might have missed from WIRED this weekCars are pretty advanced these days; some can even change lanes or park by themselves. But it turns out that the fanciest of auto buyers\u2014think the people who browse Porsche and Aston Martin showrooms\u2014are still into manual transmissions. Even when they decide to go electric.The NTSB releases its first report on bicyclist safety in nearly half a century, rankling cycling advocates with a call for mandatory helmet use.Paris tries to end an electric-scooter-share fracas with new regulations.Lamborghini rolls out a $200,000 SUV, for people who like to run errands very aggressively.Our reporter drives the hybrid Polestar 1, the first auto from Volvo\u2019s performance brand, and discovers why it\u2019s weirdly similar to the Scandinavian candy turkisk peppar.A 20-month investigation into the March 2018 crash of a self-driving Uber that killed a woman nears its end\u2014and it turns out the car wasn\u2019t trained to recognize pedestrians outside of crosswalks.Plus: If you missed WIRED25, our two-day summit in San Francisco, catch up with all our coverage right here.The award goes to this USA Today communiqu\u00e9, which reignites an extremely contentious fight and then does a dangerous thing and takes a side. The question: Is one entitled to recline one\u2019s seat on an airplane? The take: Absolutely not. \u201cSeat recline is a moral issue,\u201d one blogger tells the publication. Please forward your angry reader emails to them, not us.6.4 mphThe average speed of a New York City run commuter last year\u2014and the average speed of a New York City car moving through rush hour traffic. Yep, according to data collected by the app Strava and by the traffic analytics company Inrix, they were the same. Lace up those running shoes, New Yorkers!News from elsewhere on the internetWaymo closes its Austin office.Uber reports a $1.2 billion quarterly loss, and its shares take a tumble after the employee lockup period expires.The Department of Justice issues subpoenas to car companies that reached a compromise on auto emissions with California as part of the government\u2019s antitrust probe.GM finalizes the sale of its Lordstown plant to electric vehicle company Lordstown Motors.Uber may have to completely rebuild parts of its self-driving program or license some of its tech from Waymo after an independent expert found the company\u2019s technology uses some of the Alphabet spin-off\u2019s.A New York Times investigation finds breathalyzer tests can be unreliable.The Bloodhound LSR rocket car hits 501 mph.Here\u2019s how transit measures performed across the country on election day.Essential stories from WIRED\u2019s canonLast year, we explored why the typical way to test self-driving cars might not sufficiently account for all the ways that humans can mess up\u2014even highly trained ones who are being paid to pay attention."}{"ID": 10, "URL": "https://www.nhtsa.gov/technology-innovation/automated-vehicles-safety", "TITLE": "Automated Vehicles for Safety | NHTSA", "FILENAME": "nh-automated-vehic", "BODY": "Driver assistance\u00a0technologies in today\u2019s motor vehicles are already helping to save lives and prevent injuries.A number of today\u2019s new motor vehicles have technology that helps drivers avoid drifting into adjacent lanes or making unsafe lane changes, or that warns drivers of other vehicles behind them when they are backing up, or that brakes automatically if a vehicle ahead of them stops or slows suddenly, among other things. These and other safety technologies use a combination of hardware (sensors, cameras, and radar) and software to help vehicles identify certain safety risks so they can warn the driver to act to avoid a crash.The continuing evolution of automotive technology aims to deliver even greater safety benefits and \u2013 one day \u2013 deliver automated driving systems (ADS)\u00a0that can handle the whole task of driving when we don\u2019t want to or can\u2019t do it ourselves.Fully autonomous cars and trucks that drive us instead of us driving them will become a reality. These self-driving vehicles ultimately will integrate onto U.S. roadways by progressing through six levels of driver assistance technology advancements in the coming years. This includes everything from no automation (where a fully engaged driver is required at all times), to full autonomy (where an automated vehicle operates independently, without a human driver).\u00a0The safety benefits of automated vehicles are paramount. Automated vehicles\u2019 potential to save lives and reduce injuries is rooted in one critical and tragic fact: 94%\u00a0of serious crashes are due to human error. Automated vehicles have the potential to remove human error from the crash equation, which will help protect drivers and passengers, as well as bicyclists and pedestrians. When you consider more than 36,560\u00a0people died in motor vehicle-related crashes in the United States\u00a0in 2018, you begin to grasp the lifesaving benefits of driver assistance technologies.Automated vehicles could deliver additional economic and additional societal benefits. A NHTSA study showed motor vehicle crashes in 2010 cost $242 billion in economic activity, including $57.6 billion in lost workplace productivity, and $594 billion due to loss of life and decreased quality of life due to injuries. Eliminating the vast majority of motor vehicle crashes could erase these costs.Roads filled with automated vehicles could also cooperate to smooth traffic flow and reduce traffic congestion. Americans spent an estimated 6.9 billion hours in traffic delays in 2014, cutting into time at work or with family, increasing fuel costs and vehicle emission. With automated vehicles, the time and money spent commuting could be put to better use. A recent study stated that automated vehicles could free up as much as 50 minutes each day that had previously been dedicated to driving.While its full societal benefits are difficult to project, the transformative potential of automated vehicles and their driver assistance features can also be understood by reviewing U.S. demographics and the communities these technologies could help to support.For example, automated vehicles may also provide new mobility options to millions more Americans. Today there are 49 million Americans over age 65 and 53 million people have some form of disability.In many places across the country employment or independent living rests on the ability to drive. Automated vehicles could extend that kind of freedom to millions more. One study suggests that automated vehicles could create new employment opportunities for approximately 2 million people with disabilities.Automated or \u201cself-driving\u201d vehicles are a future technology rather than one that you\u2019ll find in a dealership tomorrow or in the next few years. A variety of technological hurdles have to be cleared, and other important issues must be addressed before these types of vehicles can be available for sale in the United States. The Department of Transportation is committed to supporting the innovators who are developing these types of vehicles to ensure their safe testing and deployment before they are available to consumers.Automated vehicles and driver assisting technologies (including those already in use on the roads) have the potential to reduce crashes, prevent injuries, and save lives. Of all serious motor vehicle crashes, 94 percent are due to human error or choices. Fully automated vehicles that can see more and act faster than human drivers could greatly reduce errors, the resulting crashes, and their toll.There is no vehicle currently available for sale that is \u201cself-driving.\u201d Every vehicle currently for sale in the United States requires the full attention of the driver at all times for safe operation. While an increasing number of vehicles now offer some automated safety features designed to assist the driver under specific conditions, there is no vehicle currently for sale that is fully automated or \u201cself-driving.\u201dMany vehicles today include safety features that assist drivers in specific circumstances, such as keeping us from drifting out of our lane or helping us stop in time to avoid a crash or reduce its severity. Read more about on this on NHTSA's safety technologies topic. If you\u2019re currently shopping for a new vehicle, review NHTSA\u2019s 5-Star Safety Ratings to make informed decisions about the safety features included in your new vehicle. If you're currently shopping for a new vehicle, review NHTSA's 5-Star Safety Ratings to make informed decisions about the safety features in your new vehicle.Vehicles are tested by the companies that build them. Companies must comply with Federal Motor Vehicle Safety Standards and certify that their vehicle is free of safety risks. Many companies today are testing advanced automated vehicles to ensure that they operate as intended, but a great deal of work remains to be done to ensure their safe operation before they are made publicly available.Cybersecurity is a critical issue that DOT and automotive companies are working to address for the future safe deployment of these technologies. Advanced vehicle safety technologies depend on an array of electronics, sensors, and computing power. In advancing these features and exploring the potential of fully autonomous vehicles, DOT and NHTSA are focused on cybersecurity to ensure that these systems work as intended. You can read more about our approach by visiting NHTSA's vehicle cybersecurity topic.These are among many important questions beyond the technical considerations that policymakers are working to address before automated vehicles are made available. We are still many years from fully automated vehicles becoming available to the public.NHTSA, under the leadership of Secretary of Transportation Elaine L. Chao, demonstrates its dedication to saving lives on our nation\u2019s roads and highways through its proactive and inclusive approach to the safe development, testing, and deployment of new and advanced vehicle technologies that have enormous potential for improving safety and mobility for all Americans, NHTSA demonstrates its dedication to saving lives on our nation\u2019s roads and highways.In September 2016, NHTSA and the U.S. Department of Transportation issued the Federal Automated Vehicles Policy which set forth a proactive approach to providing safety assurance and facilitating innovation. Building on that policy and incorporating feedback received through public comments, stakeholder meetings, and Congressional hearings, in September 2017, the agency issued, Automated Driving Systems: A Vision for Safety 2.0. The updated guidance, 2.0, offers a flexible, nonregulatory approach to automated vehicle technology safety, by supporting the automotive industry and other key stakeholders as they consider and design best practices for the safe testing and deployment of ADS levels 3 through 5. It also provides technical assistance to states and best practices for policymakers regarding ADS.In October 2018, U.S. DOT released Preparing for the Future of Transportation: Automated Vehicles 3.0, which builds upon \u2014 but does not replace \u2014 the voluntary guidance provided in 2.0. AV 3.0 expands the scope to all surface on-road transportation systems, and was developed through input from a diverse set of stakeholder engagements throughout the nation. AV 3.0 is structured around three key areas:Ensuring American Leadership in Automated Vehicle Technologies: Automated Vehicles 4.0 was released in January 2020. AV 4.0 builds upon AV 3.0 by expanding the scope to 38 relevant United States Government (USG) components that\u00a0have direct or tangential equities in safe development and integration of AV technologies. AV 4.0 is structured around three key areas:\u00a0As automated technologies advance, so will the department\u2019s guidance. The guidance is intended to be flexible and to evolve as technology does, but with safety always as the top priority."}{"ID": 11, "URL": "https://www.ghsa.org/issues/autonomous-vehicles", "TITLE": "Autonomous Vehicles | GHSA", "FILENAME": "gh-autonomous-vehi", "BODY": "It is widely acknowledged that fully autonomous vehicles (AVs), or cars and trucks that can drive themselves without a human at the controls, are coming soon. In fact, Levels 1 and 2 AVs are already on our roads.\u00a0Many companies are currently testing AVs, and AV programs have been launched across the country by various companies in the technology and transportation\u00a0industries.\nAVs have the potential for tremendous safety benefits, but it will be decades\u00a0until all vehicles on the road will be autonomous, and perhaps they never will. Until then, autonomous vehicles will need to share the road safely with human drivers.\nGHSA Policy\nClick here to view GHSA's Policy and Priorities on Autonomous Vehicles.\nFurther, the public remains skeptical about the safety of autonomous vehicles. Surveys have shown that fewer\u00a0than 30% of people report that they would feel comfortable riding in a highly automated vehicle, and fewer than a quarter would buy one as soon as it became available.\nThis presents a challenge to states, which are responsible for public education, driver licensing, and establishing and enforcing traffic\u00a0laws. GHSA believes that states should play a prominent role in dealing with the issues that will come from a mix of autonomous and human-driven vehicles on the roads.\nAutomated Vehicle Safety Expert Panel: Engaging Drivers and Law Enforcement\nOn May 8, 2019, GHSA and State Farm\u00ae hosted an\u00a0expert panel meeting in Washington, D.C., to discuss traffic safety education and law enforcement amid\u00a0the advent of automated vehicle technology. This white paper summarizes the outcomes of that meeting and provides a number of recommendations for State Highway Safety Offices and the broader safety community. Former senior NHTSA official Dr. Jim Hedlund authored the paper.\nThe white paper is available here.\nAutonomous Vehicle Policy\nAs both state and federal governments consider autonomous vehicle policy, GHSA is committed to ensuring that safety remains a priority and that states are given a voice\u00a0in these discussions. Seventeen\u00a0states and the District of Columbia have already enacted laws regarding AVs, and another six have executive orders addressing the technology.\nIn July 2017, GHSA along with the National Governors Association (NGA), the National Conference of State Legislatures (NCSL), the American Association of State Highway and Transportation Officials (AASHTO) and\u00a0the American Association of Motor Vehicle Administrators (AAMVA) sent a\u00a0letter\u00a0to members of the U.S. House of Representatives, urging the federal government to work with states to craft AV policy.\nView the Letter\nNews tagged with Autonomous Vehicles\nRelated News\nNew Report: Expert Panel Agrees There is Much to Be Done to Keep Drivers Safe in the Automated Vehicle AgeAugust 1, 2019\nGHSA Joins PAVE Coalition to Increase Driver Knowledge about Automated Vehicle SafetyJuly 9, 2019\nTraffic Safety Recommendations for Automated Vehicles Being DevelopedApril 29, 2019\nFeatured Initiative\nCurrently, there is no featured initiative on this issue.All State Highway Safety Showcases can be viewed here.\nPublications\nHighlights of Association Activity, FY 2019\nAutomated Vehicle Safety Expert Panel: Engaging Drivers and Law Enforcement\nHighlights of Association Activity, FY 2018\nView All Publications\nLaws\nThere are currently 37 states along with the District of Columbia that have enacted legislation or issues executive orders regarding Autonomous Vehicles (AVs).\nSee All Related"}{"ID": 12, "URL": "https://www.vox.com/recode/2019/5/17/18564501/self-driving-car-morals-safety-tesla-waymo", "TITLE": "Self-driving cars safety: How safe is safe enough? - Vox", "FILENAME": "vo-self-driving-ca", "BODY": "One of the biggest questions surrounding self-driving cars isn\u2019t a technological one, but instead philosophical: How safe is safe enough? It\u2019s not something for which there\u2019s an easy answer.Ever since the 2004 challenge that kicked off the autonomous vehicle push, excitement about the prospect of fleets of self-driving cars on the road has grown. Multiple companies have gotten into the game, including tech giants Google (with Waymo), Uber, and Tesla; more traditional automakers, such as General Motors, Ford, and Volvo have joined the fray. The global autonomous vehicle market is valued at an estimated $54 billion \u2014 and is projected to grow 10-fold in the next seven years. As with any innovation, self-driving cars bring with them a lot of technical issues, but there are moral ones as well. Namely, there are no clear parameters for how safe is considered safe enough to put a self-driving car on the road. At the federal level in the United States, guidelines in place are voluntary, and across states, the laws vary. If and when parameters are defined, there\u2019s no set standard for measuring whether they\u2019re met.Human-controlled driving today is already a remarkably safe activity \u2014 in the United States, there is approximately one death for every 100 million miles driven. Self-driving cars would, presumably, need to do better than that, which is what the companies behind them say they will do. But how much better isn\u2019t an easy answer. Do they need to be 10 percent safer? 100 percent safer? And is it acceptable to wait for autonomous vehicles to meet super-high safety standards if it means more people die in the meantime?Testing safety is another challenge. Gathering enough data to prove self-driving cars are safe would require hundreds of millions, even billions, of miles to be driven. It\u2019s a potentially enormously expensive endeavor, which is why researchers are trying to figure out other ways to validate driverless car safety, such as computer simulations and test tracks. Different actors within the space have different theories of the case on data gathering to test safety. As The Verge pointed out, Tesla is leaning into the data its cars already on the road are producing with its autopilot feature, while Waymo is combining computer simulations with its real-world fleet.\u201cMost people say, in a loose manner, that autonomous vehicles should be at least as good as human-driven conventional ones,\u201d Marjory Blumenthal, a senior policy researcher at research think tank RAND Corporation, said, \u201cbut we\u2019re having trouble both expressing that in concrete terms and actually making it happen.\u201dTo lay some ground work, there are six levels of autonomy established for self-driving cars, ranging from 0 to 5. A Level 0 car has no autonomous capabilities \u2014 a human driver just drives the car. A Level 4 vehicle can pretty much do all the driving on its own, but in certain conditions \u2014 for example, in set areas, or when the weather is good. A Level 5 vehicle is one that can do all the driving in all circumstances, and a human doesn\u2019t have to be involved at all. Right now, the automation systems that are on the road from companies such as Tesla, Mercedes, GM, and Volvo, are Level 2, meaning the car controls steering and speed on a well-marked highway, but a driver still has to supervise. By comparison, a Honda vehicle equipped with its \u201cSensing\u201d suite of technologies, including adaptive cruise control, lane keeping assistance, and emergency braking detection, is a Level 1.So when we\u2019re talking about completely driverless cars, that\u2019s a Level 4 or a Level 5. Daniel Sperling, founding director of the Institute of Transportation Studies at the University of California, Davis, told Recode that fully driverless cars \u2014 which don\u2019t require anyone in the car at all and can go anywhere \u2014 are \u201cnot going to happen for many, many decades, maybe never.\u201d But driverless cars in a preset \u201cgeofenced\u201d area are possible in a few years, and some places already have slow-moving, self-driving shuttles in very restricted areas.To be sure, some in the industry insist that an era of completely self-driving cars all over the roads is closer. Tesla has released videos of cars driving themselves from one destination to another and parking unassisted, though a human driver is present. But maybe don\u2019t take Tesla CEO Elon Musk\u2019s promise of 1 million completely self-driving taxis by next year very seriously. Human-controlled driving in the US today is already a relatively safe activity, though there is obviously a lot of room for improvement \u2014 37,000 people died in motor vehicle crashes in 2017, and road incidents remain a leading cause of death. So if we are going to get fleets of self-driving cars on the road, we want them to be safer. And that doesn\u2019t just mean self-driving technology \u2014 safety gains are also being made because cars are heavier, have airbags and other safety equipment, brake better, and roll over less frequently. Still, when it comes to how much safer exactly we want a driverless car to be, that is an open question.\u201cHow many millions of miles should we drive then before we\u2019re comfortable that a machine is at least as safe as a human?\u201d Greg McGuire, the director of the MCity autonomous vehicle testing lab at the University of Michigan, told me in a recent interview. \u201cOr does it need to be safer? Does it need to be 10 times as safe? What\u2019s our threshold?\u201dA 2017 study from RAND Corporation found that the sooner highly automated vehicles are deployed, the more lives will ultimately be saved, even if the cars are just slightly safer than cars driven by humans. Researchers found that in the long term, deploying cars that are just 10 percent safer than the average human driver will save more lives than waiting until they are 75 percent or 90 percent better. In other words, while we wait for self-driving cars to be perfect, more lives could be lost. Beyond what counts as safe, there is also a conundrum around who is responsible when something goes wrong. When a human driver causes an accident or fatality, there is often little doubt about who\u2019s to blame. But if a self-driving car crashes, it\u2019s not so simple. Sperling compared the scenario to another type of transportation. \u201cIf a plane doesn\u2019t have the correct software and technology in it, then who\u2019s responsible? Is it the software coder? Is it the hardware? Is it the company that owns the vehicle?\u201d he said.We\u2019ve already seen the liability question on self-driving cars play out after an Uber testing vehicle hit and killed a woman in 2018. The incident caused a media uproar, and Uber reached a settlement with the victim\u2019s family. The family of a man killed while driving a Tesla in 2018 sued the automaker earlier this year, saying that its autopilot feature was at fault, and the National Transportation Safety Board said in a preliminary report that Tesla\u2019s autopilot was active in a fatal Florida crash in March. The family of a man who died in a 2016 Tesla crash, however, has said it doesn\u2019t blame him or the company. There is also a debate about what choices the vehicles should make if faced with a tough situation \u2014 for example, if an accident is unavoidable, should a self-driving car veer onto a pedestrian-filled sidewalk, or run into a pole, which might pose more danger to the people in the vehicle? The MIT Media Lab launched a project, dubbed the \u201cMoral Machine,\u201d to try to use data to figure out how people think about those types of tradeoffs. It published a study about its findings last year. \u201c[In] cases where the harm cannot be minimized any more but can be shifted between different groups of people, then how do we want cars to do that?\u201d Edmond Awad, one of the researchers behind the study, said.But as Vox\u2019s Kelsey Piper explained at the time, these moral tradeoff questions, while interesting, don\u2019t really get to the heart of the safety debate in self-driving cars:[The] entire \u201cself-driving car\u201d setup is mostly just a novel way to bring attention to an old set of questions. What the MIT Media Lab asked survey respondents to answer was a series of variants on the classic trolley problem, a hypothetical constructed in moral philosophy to get people to think about how they weigh moral tradeoffs. The classic trolley problem asks whether you would pull a lever to move a trolley racing towards five people off-course, so instead it kills one. Variants have explored the conditions under which we\u2019re willing to kill some people to save others.It\u2019s an interesting way to learn how people think when they\u2019re forced to choose between bad options. It\u2019s interesting that there are cultural differences. But while the data collected is descriptive of how we make moral choices, it doesn\u2019t answer the question of how we should. And it\u2019s not clear that it\u2019s of any more relevance to self-driving cars than to every other policy we consider every day \u2014 all of which involve tradeoffs that can cost lives.At the time the study was released in Nature, Audi said it could help start a discussion around self-driving car decision-making, while others, including Waymo, Uber, and Toyota, stayed mum.The harder question to answer when it comes to self-driving car safety might actually be how to test it. There were 1.16 fatalities for every 100 million miles driven in the US in 2017. That means self-driving cars would have to drive hundreds of millions of miles, even hundreds of billions, to demonstrate their reliability. Waymo last year celebrated its vehicles driving 10 million miles on public roads since its 2009 launch.Accumulating billions of miles of test driving for self-driving cars is an almost impossible endeavor. Such a project would be hugely expensive and time-consuming \u2014 by some estimates, taking dozens or even hundreds of years. Plus, every time there\u2019s a change to the technology, even if it\u2019s just a couple of lines of code, the testing process would, presumably, have to start all over again. \u201cNobody could ever afford to do that,\u201d Steven Shladover, a retired research engineer at the University of California Berkeley, said. \u201cThat\u2019s why we have to start looking for other ways of reaching that level of safety assurance.\u201dIn 2018, RAND proposed a framework for measuring safety in automated vehicles, which includes testing via simulation, closed courses, and public roads with and without a safety driver. It would take place at different stages \u2014 when the technology is being developed, when it\u2019s being demonstrated, and after it\u2019s deployed. As RAND researcher Blumenthal explained, crash testing under the National Highway Traffic Safeway Administration\u2019s practices focuses on vehicle impact-resistance and occupant protection, but \u201cthere is a need to test what results from the use of software that embodies the automation.\u201d Companies do that testing, but there\u2019s no broad, agreed-upon framework in place.MCity at the University of Michigan in January released a white paper laying out safety test parameters it believes could work. It proposed an \u201cABC\u201d test concept of accelerated evaluation (focusing on the riskiest driving situations), behavior competence (scenarios that correspond to major motor vehicle crashes), and corner cases (situations that test limits of performance and technology). On-road testing of completely driverless cars is the last step, not the first. \u201cYou\u2019re mixing with real humans, so you need to be confident that you have a margin of safety that will allow you not to endanger others,\u201d McGuire, from MCity, said.Even then, where the cars are being tested makes a difference. The reason so many companies are testing their vehicles in places such as Arizona is it\u2019s relatively flat and dry \u2014 in more varied landscapes or inclement weather, vehicle detection and other autonomous capabilities become more complex and less dependable. In November 2018, Waymo CEO John Krafcik said even he doesn\u2019t think self-driving technology will ever be able to operate in all possible conditions without some human interaction. He also said that he believes it will be decades before autonomous cars are ubiquitous.\n\u201cIf you listen to some of the public pronouncements, most companies have become much more modest over time as they encounter real-world problems,\u201d Blumenthal said.It\u2019s not just researchers, engineers, and corporations in the self-driving car sector that are working on parameters for defining and measuring safety \u2014 there\u2019s a role for regulators to play. In the US, there\u2019s not much of a regulatory framework in place right now, and policy on the matter is an unanswered question.Regulators are still trying to determine what sort of data they can realistically expect to get and analyze in order to evaluate self-driving car safety. Shladover explained that another part of the problem is how we\u2019ve historically handled laws and regulations around driving in the US. At the federal level, the National Highway Traffic Safety Administration is in charge of setting vehicle safety standards and setting regulations for equipment and what gets built into vehicles. It falls under the aegis of the Department of Transportation, which is in the executive branch. In 2018, a NHTSA rule went into effect that requires new cars to have rearview technology. The rule stems from legislation enacted by Congress in 2008.States, however, typically regulate driving behavior \u2014 setting speed limits, licensing drivers, etc. \u2014 and cities and municipalities can enact rules of their own, including around driverless cars. Self-driving car systems cut across the traditional boundaries between federal, state, and city government.\u201cSome of the driving behavior is actually embedded inside the vehicle, and that would normally be a federal responsibility, but the driving behavior and the interaction with other drivers is a state responsibility,\u201d Shladover said. \u201cIt gets confused and complicated at that point.\u201dThe NHTSA is currently seeking public comment on whether cars without steering wheels or brake pedals should be allowed on the road. (They\u2019re currently prohibited, though companies can apply for exceptions.) There was a push in Congress last year to pass self-driving legislation, but it fell short. But federal, state, and local governments are still trying to figure out how to ensure safe behavior of automated driving systems and who should be in charge of it.But putting in place guidelines and companies ensuring the public that self-driving technology is safe is essential in driving the technology forward. \u201cSocial trust of these systems and how these companies are operating is as important as the engineering, if not more,\u201d McGuire said. Self-driving cars \u2014 in their limited use \u2014 and automated technology have proven to be very safe thus far, but they\u2019re not foolproof. The question we have to answer as a society is how we define safe, both in what it means and how we prove it. The idea of putting your life in the hands of a camera and a car is a daunting one, even if it is indeed safer. We\u2019re accustomed to the idea that sometimes accidents happen, and a human error can cause harm or take a life. But to grapple with a technology and a corporation doing so is perhaps more complicated. Boeing\u2019s planes are still very safe, but after a pair of crashes that might be tied to one of its automated systems, its entire fleet of 737 MAX planes has been grounded. Yes, there\u2019s a need to think about self-driving Tesla and Waymo cars rationally instead of based on fear, but it\u2019s understandable to not be wary about the idea that a line of code could kill us.Sperling told me he thinks Wall Street could play a role in improving safety \u2014 namely, investors aren\u2019t going to back a company whose vehicles they deem unsafe. \u201cIf you build a car that has multiple flaws in it that leads to deaths, you\u2019re not going to be in business very long,\u201d he said. It\u2019s in the interest of Tesla, Waymo, GM, and everyone involved to get the safety question right. They have invested a lot in self-driving and automated technology, and they have made a lot of advancements. Cars with self-driving capabilities are an increasing reality, and that\u2019s likely to only grow. Recode and Vox have joined forces to uncover and explain how our digital world is changing \u2014 and changing us. Subscribe to Recode podcasts to hear Kara Swisher and Peter Kafka lead the tough conversations the technology industry needs today."}{"ID": 13, "URL": "https://www.curbed.com/2016/9/21/12991696/driverless-cars-safety-pros-cons", "TITLE": "Driverless cars: Are they safe? - Curbed", "FILENAME": "cu-driverless-cars", "BODY": "From ushering in an era of decreased car ownership, to narrowing streets and eliminating parking lots, autonomous vehicles promise to dramatically reshape our cities.But after an Uber-operated self-driving vehicle struck and killed 49-year-old Elaine Herzberg, who was crossing the street with her bike in Tempe, Arizona on March 18, 2018, there are more questions than ever about the safety of this technology, especially as these vehicles are being tested more frequently on public streets. Some argue the safety record for self-driving cars isn\u2019t proven, and that it\u2019s unclear whether or not enough testing miles have been driven in real-life conditions. Other safety advocates go further, and say that driverless cars are introducing a new problem to cities, when cities should instead be focusing on improving transit and encouraging walking and biking instead.Contentions aside, the autonomous revolution is already here, although some cities will see its impacts sooner than others. From Las Vegas, where a Navya self-driving minibus scoots slowly along a downtown street, to General Motors\u2019 Cruise ride-hailing service in San Francisco with backup humans in the driver\u2019s seat, to Waymo\u2019s family-focused Chandler, Arizona\u2013based pilot program that uses no human operators in its Chrysler Pacifica minivans at all, the country is accelerating towards a driverless future.While the U.S. government has historically been confident in autonomous vehicles\u2019 ability to end the epidemic of traffic deaths on our streets, there are plenty of concerns from opponents of self-driving cars that are making cities think twice before welcoming them to their streets.In 2009, Google launched its self-driving project focusing on saving lives and serving people with disabilities. In a 2014 video, Google showed blind and elderly riders climbing into its custom-designed autonomous vehicles, part of the company\u2019s plan to \u201cimprove road safety and help lots of people who can\u2019t drive.\u201dAlthough there were several self-driving projects in the country at the time, many being developed by government agencies or university labs, Google\u2019s project differentiated itself by being public-facing. The goal was not to build cars\u2014although Google did build its own testing prototypes\u2014but to create a self-driving service that would help regular people get around. Google began testing its vehicles on public streets the very same year the project launched. With the reorganization of Google into its new parent company Alphabet, the self-driving program became its own entity, Waymo. Almost a decade later, Waymo remains the clear leader for safe self-driving miles on U.S. streets. As of November 2019, Waymo had logged 10 million self-driven miles, making it the leader for self-driven miles on U.S. streets. That\u2019s double the amount of miles Waymo had driven in February 2018. In July 2018, Waymo\u2019s new electric Jaguar I-Paces vehicle hit the streets, in addition to its Chrysler Pacifica hybrid minivans. According to Waymo\u2019s monthly reports, its vehicles have been in dozens of crashes, but caused no serious injuries. In 2016, a Waymo vehicle bumped a bus while going 2 miles per hour. On May 4, 2018, one of Waymo\u2019s minivans was involved in a crash with minor injuries in Chandler, Arizona while in autonomous mode, but police said Waymo\u2019s van was not the \u201cviolator vehicle.\u201d In October 2018 a Waymo vehicle was involved in a crash that sent a motorcyclist to the hospital with minor injuries, but the human driver was at fault.Now there are dozens of autonomous vehicle companies testing on U.S. streets, but next most experienced companies, Uber and GM Cruise, are still several million miles behind Waymo. That doesn\u2019t include miles driven in the semi-autonomous modes that many cars now offer, like Tesla\u2019s Autopilot, which are more driver-assistance systems than true self-driving vehicles. In the last few years, the greatest strides taken in the self-driving industry have been by ride-hailing companies, who are devoting an exceptional amount of time and money to develop their own proprietary technologies and, in many cases, giving members of the public rides in their vehicles. In 2017, Lyft\u2019s CEO predicted that within five years, all their vehicles will be autonomous. At a press conference in March 2018, where Waymo\u2019s CEO John Krafcik announced its ride-hailing program, Krafcik claimed that the company will be making at least one million trips per day by 2020.The biggest safety advantage to an autonomous vehicle is that a robot is not a human\u2014it is programmed to obey all the rules of the road, won\u2019t speed, and can\u2019t be distracted by a text message flickering onto a phone. And, hypothetically at least, AVs can also detect what humans can\u2019t\u2014especially at night or in low-light conditions\u2014and react more quickly to avoid a collision.AVs are laden with sensors and software that work together to build a complete picture of the road. One key technology for AVs is LIDAR, or a \u201clight-detecting and ranging\u201d sensor. Using millions of lasers, LIDAR draws a real-time, 3D image of the environment around the vehicle. In addition to LIDAR, radar sensors can measure the size and speed of moving objects. And high-definition cameras can actually read signs and signals. As the car is traveling, it cross-references all this data with GPS technology that situates the vehicle within a city and helps to plan its route.In addition to the sensors and maps, AVs run software programs which make real-time decisions about how the car will navigate relative to other vehicles, humans, or objects in the road. Engineers can run the cars through simulations, but the software also needs to learn from actual driving situations. This is why real-world testing on public roads is so important. But how AV companies gather that information has led to greater concerns about how autonomous vehicles can detect and avoid vulnerable road users, like cyclists, and people who move slowly and more erratically through streets, like seniors and children. Waymo, for example, claims its software has been explicitly programmed to recognize cyclists. A video that Waymo released in 2016 (back when it was still part of Google) shows how one of its vehicles detected and stopped for a wrong-way cyclist coming around a corner at night.According to a May 2018 report from The Information, Uber\u2019s vehicle did detect Herzberg before its fatal Tempe crash, but the system made a decision not to swerve. \u201cThe car\u2019s sensors detected the pedestrian, who was crossing the street with a bicycle, but Uber\u2019s software decided it didn\u2019t need to react right away.\u201d A preliminary report by the National Transportation Safety Board (NTSB) confirmed that not only had Uber disabled the Volvo SUV\u2019s collision-avoidance feature, Uber\u2019s own system detected Herzberg six seconds before the crash and did not brake until 1.3 seconds before impact. However, Arizona prosecutors did not charge Uber, writing in a letter that \u201cthere is no basis for criminal liability for the Uber corporation arising from this matter.\u201dUber\u2019s self-driving unit Arizona announced it was closing down on May 23, 2018. In July, Uber eliminated 100 self-driving positions in Pittsburgh and San Francisco. Uber\u2019s self-driving program has since returned to Pittsburgh, where it is limiting testing to daylight hours.The role of human \u201cbackup drivers\u201d as part of AV testing has also come into question after Tempe police documents obtained by Gizmodo showed that driver Rafaela Vasquez was streaming a video on her phone at the time of Uber\u2019s fatal crash. \u201cThe driver in this case could have reacted and brought the vehicle to a stop 42.61 feet prior to the pedestrian,\u201d reads the report, which calls the crash \u201centirely avoidable.\u201d In November 2019, the final NTSB report attributed the crash to human error, but placed much of the blame on Vasquez. At the NTSB hearing, board members also criticized the federal government for safety regulations that are too lax.Self-driving companies also put their vehicles through endless tests using simulated city streets. Many traditional automakers use a facility named M City in Ann Arbor, Michigan, but the larger self-driving companies have built their own fake cities specifically to test interactions with humans who are not in vehicles. Waymo\u2019s fake city, named Castle, even has a shed full of props\u2014like tricycles\u2014that might be used by people on streets so that Waymo\u2019s engineers can learn how to identify them.After Uber\u2019s fatal crash, Toyota built a new facility to test its vehicles\u2019 responses to \u201cedge cases\u201d\u2014extreme situations too dangerous to test on public streets.50 years ago, the U.S.\u2019s rate of traffic deaths was higher than they are now\u2014in 1980, generally considered to be the deadliest year on U.S. streets, over 50,000 people were killed. With safety features like airbags added to vehicles, stricter seat belt laws, and campaigns that stigmatized drunk driving, the rate of deaths went down significantly.But over the last few years, the U.S. has seen a slight increase in traffic deaths again. Additionally, pedestrian fatalities increased by 27 percent over the last decade, while all other traffic fatalities decreased by 14 percent. There isn\u2019t agreement for why these deaths are increasing, but some experts believe that this is because Americans are driving more\u2014overall vehicle-miles traveled (VMT) reached an all-time high in 2017. Using USDOT\u2019s claim that 94 percent of crashes are caused by human error, it seems like a fairly obvious way to reduce crashes is to reduce the number of humans behind the wheel. But it\u2019s not just the number of human drivers that should be reduced, the U.S. could also reduce the number of cars on roads to prevent fatalities\u2014and autonomous vehicles might be able to help do that, too.The real safety promise of autonomous vehicles is the fact that these vehicles can be be summoned on-demand, routed more efficiently, and easily shared\u2014meaning not just the overall number of single-passenger cars on streets will decline, but the number of single-passenger trips will be reduced, meaning a reduction in overall miles traveled.In addition, cities can use automated vehicles to tackle ambitious on-demand transit projects, like a proposed initiative to integrate shared self-driving vehicles into the public transit fleet. If cities can launch these kind of \u201cmicrotransit\u201d systems that serve as a first-mile/last-mile solution to help get more people to fixed-route public transportation, that will also mean fewer people in cars and more people on safer modes of transit. According to a 2016 American Public Transportation Association study, traveling by public transportation is ten times safer per mile than traveling by car.Without having to make room for so many cars, city streets can be narrowed, making even more room for pedestrians and bikes to safely navigate cities. In this way, autonomous vehicles have a great role to play as part of a Vision Zero strategy, which most major U.S. cities have implemented in order to eliminate traffic deaths.While residents of only a few cities can summon an AV on-demand right now, the truth is that much of the safety tech powering self-driving cars is making its way into today\u2019s cars. Sophisticated collision-avoidance systems, for example, which can stop a vehicle if an object or person are detected in its path, are already being incorporated into new cars and buses. This is why the way the National Highway Traffic Safety Administration (NHTSA) tests those kinds of safety innovations is also changing. Until recently, all safety standards were based on historical crash data, meaning the government had to track years and years of roadway incidents (and, in many cases, deaths) before making an official recommendation. Now, technology is advancing so quickly that there\u2019s not enough time to test every new idea for a decade. The government knows it needs to be more nimble.In fact, that\u2019s what happened for a recent USDOT recommendation that all cars be equipped with vehicle-to-vehicle communication (V2V), a tool which allows cars to \u201ctalk\u201d to each other. This recommendation was fast-tracked in 2015 by U.S. transportation secretary Anthony Foxx after detailed simulations and modeling showed that the benefits were obvious\u2014there was no need to spend years collecting historical data. The same type of recommendation might be made for an aspect of autonomous tech. Once a clear safety benefit has been proven across the self-driving industry, a specific feature might become standard on all vehicles.About half of U.S. states allow testing of autonomous vehicles on public roads, but regulations for each state vary widely. The majority of testing is focused in a handful of states: Arizona, California, Georgia, Michigan, Nevada, Texas, Pennsylvania, and Washington. California remains the busiest hub for the AV industry: There are currently 52 companies testing self-driving technology on the state\u2019s streets. It\u2019s also one of the most heavily regulated markets: California\u2019s Department of Motor Vehicles requires companies to file for a permit and submit annual reports that include the number of miles driven and any crashes. While it\u2019s not necessarily used as a safety metric, one performance standard that helps to illustrate how technology is improving is tracking the number of times per self-driving mile that a human driver has to take over, which is called a \u201cdisengagement.\u201d California DMV records demonstrate that as self-driving programs log more on-road experience, they see fewer and fewer disengagements.Other states don\u2019t require as much documentation as California\u2014and they\u2019re not necessarily required to make any information public. Arizona, for example, approved AV testing on public roads in 2016 without notifying its residents, and didn\u2019t require any reports from companies, although after Uber\u2019s fatal crash, that has changed.In 2016, the U.S. government released its long-awaited rules on self-driving vehicles. The Department of Transportation\u2019s 116-page document lists many benefits for bringing technology to market, among them improved sustainability, productivity, and accessibility. But the USDOT report\u2019s central promise is that autonomy will pave the way for policies that dramatically improve road safety.Even President Obama made the case for safety in an op-ed that heralded the dawn of the new driverless age:Right now, too many people die on our roads\u201435,200 last year alone\u2014with 94 percent of those the result of human error or choice. Automated vehicles have the potential to save tens of thousands of lives each year. And right now, for too many senior citizens and Americans with disabilities, driving isn\u2019t an option. Automated vehicles could change their lives.In order to get cities across the country to start thinking about using autonomy to solve transportation problems, USDOT hosted the Smart City Challenge in 2016, which awarded $40 million to Columbus, Ohio, to develop a fleet of autonomous transit vehicles. As a result of the challenge, the 70 cities that competed now have blueprints for how to introduce AV tech to their transportation planning.Under the Trump administration, much of the legislation proposed has been centered around exemptions for automakers and increasing the number of AVs allowed to operate on U.S. streets. In fact, in September 2017, USDOT and NHTSA issued updated AV guidelines, which carried an even lighter regulatory touch, after industry leaders expressed concerns about regulation at the federal level stifling innovation.In addition to the 2017 policy statement, Transportation Secretary Elaine Chao held preliminary hearings about autonomous vehicles where she affirmed the government would not play a heavy-handed role. \u201cThe market will decide what is the most effective solution,\u201d she said. However, the aggressive development of V2V\u2014which experts agree can work to make human-driven cars much safer as autonomous technology comes to market\u2014has not been made a priority during her leadership.On October 4, 2018, USDOT announced new guidelines to guide development and deployment of AVs, including the possibility that the department would change its safety standard rules to allow new types of autonomous vehicles to operate on U.S. roads. This would mean potentially making exemptions for automakers to produce vehicles without human-centered operating features like steering wheels. Chao also addressed the fact that public acceptance was a key element for autonomous vehicle adoption. \u201cCompanies need to step up and address the public\u2019s concerns about safety,\u201d she said. \u201cBecause without public acceptance, the full potential of these technologies may never be realized.\u201dUSDOT\u2019s lack of regulation was called out as \u201claughable\u201d by NTSB board member Jennifer Homendy during the November 2019 hearings about Uber\u2019s fatal crash. \u201cI actually think that there is a major failing on the federal government\u2019s part and the state of Arizona, because they also didn\u2019t have any standards in place and still don\u2019t, for failing to regulate these operations,\u201d she said. In a speech at the Consumer Electronics Show on January 8, 2020, Chao announced AV 4.0, a new set of voluntary guidelines which reconfirms the federal government\u2019s hands-off approach to AV regulation.The plan was quickly panned by safety advocates, including Cathy Chase, president of Advocates for Highway and Auto Safety, who cited Uber\u2019s fatal crash, among others, in a statement denouncing AV 4.0. \u201cDespite these disturbing incidents, the agency tasked with ensuring public safety on our roadways has abrogated their responsibility to issue rules requiring minimum performance standards for AVs.\u201dThere\u2019s one safety debate that continues to divide the self-driving industry: Some automakers are still pushing for versions of vehicles which allow control to pass from human to computer, offering drivers the ability to toggle between semi-autonomous and fully autonomous modes. Two fatal Tesla crashes\u2014one in 2016 and one in 2018\u2014that occurred while the drivers were using the vehicle\u2019s Autopilot feature illustrated the dangers of a semi-autonomous mode. As the National Transportation Safety Board (NTSB) noted in its report of the 2016 crash, semi-autonomous systems give \u201cfar more leeway to the driver to divert his attention to something other than driving.\u201dFully autonomous is the official policy recommendation from the Self-Driving Coalition for Safer Streets, a lobbying group that wants cars to eventually phase out steering wheels and let the software take over, 100 percent of the time. This completely eliminates the potential for human error. In 2018, Waymo began conducting fully autonomous testing in Arizona without a human safety driver, and in 2019, began transporting passengers in fully autonomous vehicles. California now allows fully autonomous testing as well, but without passengers for now. Especially after the Uber crash, San Francisco bike advocates worry that the tech isn\u2019t powerful enough to see cyclists. The California Bicycle Coalition started a petition to stop fully autonomous vehicles from being tested on California streets.At least for the near future, even fully autonomous vehicles will still have to contend with the mistakes of human drivers. To truly make self-driving technology the safest it can be, all the vehicles on the road should be fully autonomous\u2014not just programmed to obey the rules of the road, but also to communicate with each other. In 2017, National Association of City Transportation Officials (NACTO) created a Blueprint for Autonomous Urbanism, which encourages cities to deploy fully autonomous vehicles that travel no faster than 25 mph as a tool for making streets safer, \u201cwith mandatory yielding to people outside of vehicles.\u201d A dozen U.S. cities including Austin, Detroit and Columbus, Ohio, are currently testing slow-moving autonomous shuttles like this on city streets.From new street designs to accessibility guidelines to a focus on data sharing, NACTO\u2019s policy document provides the most detailed AV recommendations for U.S. urban transportation planners. To plot the safest path forward for self-driving vehicles\u2014and for cities to reap the many other environmental and social benefits of the technology\u2014AVs should provide shared rides in regulated fleets, integrate with existing transit, and operate in a way that prioritizes a city\u2019s most vulnerable humans above all users of the streets."}{"ID": 14, "URL": "https://www.curbed.com/transportation/2018/3/23/17153200/delete-uber-cities", "TITLE": "Uber\u2019s self-driving failures have set back the industry - Curbed", "FILENAME": "cu-delete-uber-cit", "BODY": "Filed under:Uber\u2019s self-driving program has never played by the rules, and now our safety is at riskUber\u2019s self-driving program arrived in Arizona under odd circumstances.About a week after the company\u2019s self-driving pilot program launched on the streets of San Francisco in December 2016, the California Department of Motor Vehicles revoked Uber vehicles\u2019 registrations because the company hadn\u2019t filed a $150 permit.Instead of following California law, Uber loaded its 16 self-driving cars onto a flatbed trailer and drove them to Arizona. As they were en route, the state\u2019s governor, Doug Ducey, issued a statement: \u201cArizona welcomes Uber self-driving cars with open arms and wide open roads,\u201d it read. \u201cWhile California puts the brakes on innovation and change with more bureaucracy and more regulation, Arizona is paving the way for new technology and new businesses.\u201dUpdate, March 26, 9:20 p.m. ET: Arizona has suspended Uber from autonomous vehicle testing. In California, Uber is letting its self-driving permit expire on March 31.The flagrant flouting of California\u2019s regulations, the dramatic, professionally photographed exodus\u2014it all seemed less like the actions of a responsible global corporation and more like a bratty kid yanking away his toys after picking a fight.\u201cIt\u2019s not about picking a fight,\u201d Anthony Levandowski, Uber\u2019s self-driving program director, said at the time. \u201cIt\u2019s about doing the right thing. And we believe that bringing this tech to California is the right thing to do.\u201dA year later, Levandowski himself was accused of not doing the right thing. Levandowski, who had worked at Uber competitor Waymo when its self-driving operation was still part of Google (it\u2019s now part of Alphabet), was named in a lawsuit for trying to steal Waymo\u2019s technology. The case was settled within a week\u2014Uber agreed to give Waymo\u2019s parent company Alphabet about $245 million in equity. But even more importantly, as Recode reported, the settlement came with a guarantee for Waymo\u2014\u201cthat Uber won\u2019t use their self-driving tech.\u201dThe technology that Waymo claims Uber was trying to steal is the technology that makes its cars much safer than any other self-driving cars on the road:One of the most powerful parts of our self-driving technology is our custom-built LIDAR\u200a\u2014\u200aor \u201cLight Detection and Ranging.\u201d LIDAR works by bouncing millions of laser beams off surrounding objects and measuring how long it takes for the light to reflect, painting a 3D picture of the world. LIDAR is critical to detecting and measuring the shape, speed and movement of objects like cyclists, vehicles and pedestrians.Now, however, the performance of Uber\u2019s own proprietary tech, and how well it detected and measured the movement of a pedestrian\u2014specifically a person walking a bike\u2014is under intense scrutiny after one of Uber\u2019s vehicles struck and killed Elaine Herzberg in Tempe, Arizona on Sunday night.Uber has poisoned the well for a nascent industry that has largely done the right thing up until now.The National Transportation Safety Board and National Highway Safety Administration are currently conducting a full investigation, and the results will likely take months. But just using the the basic information about the crash, including a dashcam video shared by Tempe police, most autonomous vehicle experts interviewed about the crash\u2014by the Wall Street Journal, Wired, the Arizona Republic\u2014agree that Uber\u2019s self-driving system failed. The technology not only failed, but Uber is also responsible for the death, writes The Drive\u2019s Alex Roy in a very strongly worded and thorough examination of Uber\u2019s culpability. \u201cEven if you believe self-driving cars may someday reduce road fatalities\u2014and I do believe that\u2014this dashcam video is an icepick in the face of the argument that anyone at Uber gives a damn about anyone\u2019s safety, including that of their own test drivers.\u201dThe safety of Uber\u2019s autonomous vehicle testing has been cited as a concern before. Mere hours after Uber\u2019s San Francisco self-driving trial began, The Verge reported that one of Uber\u2019s cars ran a red light, nearly hitting a (human-driven) Lyft car. \u201cSafety is our top priority,\u201d commented a spokesperson, who told The Verge that the error was due to a human safety driver. Yet a New York Times investigation that looked at the vehicle\u2019s internal logs found that Uber\u2019s system failed to recognize the stop light\u2014as well as several others.Uber\u2019s vehicles were then accused of driving into San Francisco\u2019s bike lanes without warning. This was not the fault of human drivers but a known software error, Uber told The Verge. But rather than fix the software in time for its public launch, Uber had told its human safety drivers simply to take control of the vehicle when turning right in a street with a bike lane.When a human driver has to take over from a self-driving car\u2019s system, it\u2019s called a \u201cdisengagement,\u201d and it\u2019s something that autonomous companies see as a last resort, as it can often be dangerous. California DMV records demonstrate that as self-driving programs log more on-road experience, they see fewer and fewer disengagements. Waymo, for example, now sees one disengagement per every 5,600 miles driven.Uber isn't showing the same trends. In fact, evidence shows an aggressive push to bring its technology to market when it clearly wasn't ready. For one, Uber has not publicly reported its own self-driving miles and disengagements. According to documents obtained by Recode last year, the vehicles are not driving enough to get the experience the software needs, resulting in a troubling number of disengagements: After driving 20,354 miles, Uber\u2019s cars had to be taken over by human drivers at every mile. The New York Times obtained newer documents which showed Uber was trying to reach a goal of 13 miles per disengagement in Arizona.Uber says it\u2019s logged millions of self-driving miles, but without that disengagement data made public, cities have no idea if the technology is getting safer.There are five major companies that recently started testing in Arizona\u2014Uber, Waymo, Ford, General Motors, and Intel\u2014all of which are also on the road testing in other cities.But of those companies, none come close to the experience logged by Waymo, which recently reached a milestone of five million self-driven miles. Waymo\u2014which has been testing its technology on California streets since 2009 (where it obtained all the proper permits) and is now in 20 other cities\u2014launched public trials in Arizona in 2017, choosing Chrysler Pacifica minivans over SUVs is because Waymo\u2019s pilot project focuses on families and people with disabilities. Earlier this month, Waymo even began conducting fully autonomous testing in Arizona without a human safety driver at all.Waymo has reported dozens of fender-benders with its vehicles but only one at-fault collision: The car was going 2 mph and bumped into a bus. We know this because Waymo publishes monthly safety reports, including a comprehensive 43-page summary in 2017. Waymo also has software that it claims has been explicitly programmed to recognize cyclists. One of the videos that Waymo released (back when it was still part of Google) shows how one of its vehicles detected and stopped for a wrong-way cyclist.In fact, Waymo built an entire city specifically to test interactions with humans who are not in vehicles. This includes regional roundabout designs frequented by cyclists and parallel parking schemes meant to mimic those in shopping and entertainment districts where people are getting in and out of cars. There\u2019s even a shed full of props\u2014like tricycles\u2014that might be used by people on streets so that Waymo\u2019s engineers can learn how to identify and avoid them.A daily dose of design and real estate news, intel, and eye candyUber is probably doing some of those things, too. But the public largely doesn\u2019t know about it. Uber is notoriously secretive about its operations\u2014and especially about its self-driving division.The list of Uber\u2019s self-driving safety concerns doesn\u2019t even include the many missteps of its ride-hailing program and internal turmoil that has forced engineers to leave the company. And unlike other transportation network companies that have formed partnerships with cities, only last year did Uber begin to incrementally share its data.Autonomous vehicle rules have yet to be developed at the federal level, but there are guidelines that have been developed from several transportation groups about how they should be safely deployed: The National Association of City Transportation Officials (NACTO) released a Blueprint for Autonomous Urbanism, and the Self-Driving Coalition for Safer Streets, of which Uber is a member, has its own policy recommendations, although the coalition has not commented on Uber\u2019s fatal crash.Unless there are more specific safety standards for testing set in place, how will we know about Uber\u2019s near-misses? How many other incidents will there be where Uber\u2019s system won\u2019t see a red light, or a bike, or a kid walking with his parents?In other cities where Uber\u2019s autonomous program is operating, but currently suspended (like Pittsburgh, Toronto, and San Francisco, where it returned last May), officials may already know the answer to these questions. But we, the people who are using these streets, definitely don\u2019t.Autonomous vehicles hold tremendous promise for reducing traffic deaths, a public health crisis that cities are already working hard to address. Now Uber\u2019s arrogance and blatant disregard for safety has set the industry back, and Uber needs to step aside so this technology can be properly developed by the dozens of companies that are acting in good faith.Take the tiny autonomous minibus that scoots around downtown Las Vegas. The bus, developed by French companies Navya and Keolis, doesn\u2019t even go faster than 15 mph because it travels in an area where the movements of pedestrians are prioritized. In fact, it\u2019s so overly cautious that when a truck started backing up into it on its very first day of service, the bus just sat there while its front bumper got crunched. No one was hurt, engineers learned how to avoid this problem in the future, and the bus was back on its route two days later.That\u2019s the kind of do-no-harm autonomous vehicle testing that we should allow on our streets.This story has been updated with an investigation by the New York Times.\nRelated\nA daily dose of design and real estate news, intel, and eye candyIt provides ample space for every single bottle, can, and jar in my pantry, along with several of small appliances.Petrus Palm\u00e9r is selling his cozy and stylish family home nestled in the center of the city.Now you can explore the designer-architect\u2019s legacy from your couch.Designed by Frank Lloyd Wright in 1951, the house first hit the market in 2018 for $3.4 million, but it\u2019s now for sale at a discount.The home was constructed in 1890 by a local New Hampshire businessman.The home centers around a cavernous living and dining area with sweeping views out back."}{"ID": 15, "URL": "https://www.rand.org/pubs/research_reports/RR1478.html", "TITLE": "Driving to Safety: How Many Miles of Driving Would It Take to Demonstrate Autonomous Vehicle Reliability? | RAND", "FILENAME": "ra-RR1478.html", "BODY": "How safe are autonomous vehicles? The answer is crucial for developing sound policies to govern their deployment. One proposal to assess safety is to test-drive autonomous vehicles in real traffic, observe their performance, and make statistical comparisons to human driver performance. This approach is logical, but is it practical? In this report, we calculate the number of miles that would need to be driven to provide clear statistical evidence of autonomous vehicle safety. Given that current traffic fatalities and injuries are rare events compared with vehicle miles traveled, we show that fully autonomous vehicles would have to be driven hundreds of millions of miles and sometimes hundreds of billions of miles to demonstrate their safety in terms of fatalities and injuries. Under even aggressive testing assumptions, existing fleets would take tens and sometimes hundreds of years to drive these miles \u2014 an impossible proposition if the aim is to demonstrate performance prior to releasing them for consumer use. Our findings demonstrate that developers of this technology and third-party testers cannot simply drive their way to safety. Instead, they will need to develop innovative methods of demonstrating safety and reliability. And yet, it may still not be possible to establish with certainty the safety of autonomous vehicles. Therefore, it is imperative that autonomous vehicle regulations are adaptive \u2014 designed from the outset to evolve with the technology so that society can better harness the benefits and manage the risks of these rapidly evolving and potentially transformative technologies.News ReleaseApr 12, 2016InfographicJan 9, 2017This research was conducted in the Science, Technology and Policy Program, a part of RAND Justice, Infrastructure, and Environment.This report is part of the RAND Corporation research report series. RAND reports present research findings and objective analysis that address the challenges facing the public and private sectors. All RAND reports undergo rigorous peer review to ensure high standards for research quality and objectivity.Permission is given to duplicate this electronic document for personal use only, as long as it is unaltered and complete. Copies may not be duplicated for commercial purposes. Unauthorized posting of RAND PDFs to a non-RAND Web site is prohibited. RAND PDFs are protected under copyright law. For information on reprint and linking permissions, please visit the RAND Permissions page.The RAND Corporation is a nonprofit institution that helps improve policy and decisionmaking through research and analysis. RAND's publications do not necessarily reflect the opinions of its research clients and sponsors.Get weekly updates from RAND."}{"ID": 16, "URL": "https://www.curbed.com/transportation/2018/3/20/17142090/uber-fatal-crash-driverless-pedestrian-safety", "TITLE": "Uber\u2019s fatal crash shows how cities prioritize cars over human lives - Curbed", "FILENAME": "cu-uber-fatal-cras", "BODY": "Filed under:Uber\u2019s fatal Arizona crash is a call to action for citiesWhat you will hear the most about Elaine Herzberg, a homeless resident of Tempe, Arizona, who was struck and killed by one of Uber\u2019s autonomous vehicles when she attempted to walk her bike across an eight-lane street, is that she was not using the crosswalk.\u201cA female walking outside of the crosswalk,\u201d noted the initial police statement. \u201cAs soon as she walked into the lane of traffic, she was struck by the vehicle,\u201d reported the sergeant giving a press conference. \u201cThe driver said it was like a flash, the person walked out in front of them,\u201d said Tempe police chief Sylvia Moir. \u201cIt is dangerous to cross roadways in the evening hour when well-illuminated, managed crosswalks are available.\u201d The chief of Tempe\u2019s police department went on to tell the San Francisco Chronicle that \u201cUber would likely not be at fault,\u201d even though the preliminary police investigation determined that the car was speeding\u2014going 38 mph in a 35 mph zone when the crash occurred. (A New York Times story says the vehicle was traveling 40 in a 45 mph zone but does not cite its sources, although a July 2017 Google Street View image shows a 45 mph sign not far from the crash site. According to Tempe police spokesperson Lily Duran, the speed limit is 35 mph.)A daily dose of design and real estate news, intel, and eye candyA video released by the police department shows Herzberg walking her bike from the center median across two vehicular lanes when she is struck by the vehicle. The human safety driver behind the wheel is shown looking down and not at the road for much of the time before the impact. Arizona has the highest rate of pedestrian deaths in the nation. Ten pedestrians were killed in the state just in the past week. Last Tuesday, three seniors were killed by a single driver in nearby Fountain Hills. They were all walking in marked crosswalks.The fact that the state is so deadly for walkers is not a coincidence. The same factor that is responsible for Arizona\u2019s high number of pedestrian deaths is the very same reason Uber is testing there\u2014the state prioritizes cars over the lives of pedestrians. Here\u2019s a picture of the intersection near where an Uber computer-driven car hit & killed a pedestrian.https://t.co/H8MJkJYMDg pic.twitter.com/zrGIA6xQIDWhen Uber was ordered to halt its autonomous testing in San Francisco for refusing to file a permit in late 2016, the company orchestrated a dramatic exodus to Arizona, where Gov. Doug Ducey welcomed Uber with the promise of \u201cwide open roads.\u201d Experts have long attributed the state\u2019s high rate of pedestrian deaths to exceptionally wide streets that are engineered to move cars fast and do not provide adequate safety infrastructure for people who are on foot or bike.The fast movement of cars is what kills pedestrians. Glancing at a phone might cause a person using a street to make a mistake that results in a collision. When a car crashes into a human, speed is what turns that collision into a death.The great promise of autonomous vehicles for road safety advocates is that they are meant to eliminate the mistakes that human drivers make. They aren\u2019t supposed to exceed the speed limit, or get distracted by sending a text, or have their vision impaired by driving at night, when most pedestrian deaths occur.Uber's autonomous vehicle was going 38 in a 35. Speeding. If you don't think going a few miles per hour over the speed limit matters, in this case it may have meant the difference between life and death https://t.co/54pJNogwLW pic.twitter.com/66gVxjH5w7Just on the stretch of North Mill Avenue where the crash occurred, there is plenty of information that should have made Uber\u2019s vehicle drive extra cautiously. There is a bike lane. There is a paved multi-use trail. There are multiple signs that say \u201cbike lane\u201d and \u201cyield to bikes.\u201d Uber\u2019s programming should have taken into consideration that on this stretch of road in particular, there will likely be people walking and biking.There will be ongoing debate about whether or not the array of sensors atop Uber\u2019s vehicle should have been able to spot a pedestrian or if the \u201csafety driver\u201d inside should have intervened. But that\u2019s not the point. Uber\u2019s vehicles are not designed to replace human drivers. They are tasked with being better than human drivers. If an autonomous vehicle is not several magnitudes better than a human driver at detecting a pedestrian and preventing the loss of human life, it cannot be on the road. Additionally, cities need to admit that their roads need to be fixed.A super-weird aspect of this crash site is that it occurred at a place where a beautiful brick-paved diagonal walking path was provided across the median, along with a sign instructing people not to use it. This is beyond pedestrian-hostile design; it's damn-near entrapment. pic.twitter.com/ZaHw9bIIrROver the last few years, two U.S. cities have made significant headway towards reducing pedestrian deaths. Due to comprehensive, well-funded initiatives, New York and San Francisco have both reduced their traffic deaths to their lowest numbers since the widespread adoption of the (human-driven) automobile.In fact, most major cities across the country now have Vision Zero strategies to eliminate fatal collisions by redesigning streets in a way that reduces potential conflict between people and cars. A city that signs on agrees that through better street design, every traffic death is preventable.Earlier this year, Tempe adopted its own Vision Zero initiative that will govern how the city addresses traffic safety. It might take a cue from Orlando, the deadliest U.S. city for pedestrians, which is dramatically redesigning its extra-wide streets to protect walkers and bikers. Here is my statement regarding the tragic event that occurred in Tempe overnight. pic.twitter.com/3ql1WAxKpABut whatever Tempe chooses to do, until those roadways are determined to be safer, the city should not let Uber back on its streets.Before autonomous vehicles can operate on any city\u2019s streets, a city should prove to its residents that it is taking every possible measure to keep them safe\u2014redesigning its streets for people, providing more accessible transit options, and reducing traffic deaths. That includes, as recommended by the National Association of City Transportation Officials (NACTO), never allowing autonomous vehicles to travel faster than 25 mph in densely populated areas\u2014the speed at which 80 percent of humans will likely live if struck by that vehicle.Crosswalk or no crosswalk, human driver or robot driver, the only acceptable number of deaths on our streets is no deaths.Autonomous mobility is only as safe as the street's most vulnerable user. Driverless cars & roads must be designed w an authentic respect for people, not just a legal or technical one. Streets aren\u2019t safe until everyone on them is safe, *especially* when they\u2019re outside the linesEach day, human drivers on U.S. streets kill at least 16 pedestrians. Among wealthy democratic countries, this makes the U.S. not just an outlier, but an anomaly. U.S. cities have a 40 percent higher rate of traffic deaths compared to our peer nations. American children are twice as likely as kids in those countries to be killed by cars.The deaths of nearly 6,000 pedestrians on American streets every year are clearly unacceptable.But the death of Elaine Herzberg is particularly unacceptable because it has now set a dangerous precedent.The failure of most local authorities to address the fact that Uber\u2019s vehicle was speeding, or Arizona\u2019s prioritization of cars, or the dangerous design of Tempe\u2019s streets as factors in this crash has now reinforced the belief of many Americans that pedestrians who are killed are \u201cdistracted walkers\u201d who deserve to be punished or ticketed or criminalized or slandered. Yet statistically, the people killed by cars in this country are our most vulnerable residents\u2014the youngest, the oldest, the sickest, the poorest, and overwhelmingly likely to be people of color. These are our parents and children. And until we decide walkers matter more than cars, many more of them will die. If we accept a city\u2019s claim that it was not able to prevent one pedestrian from being killed by an autonomous vehicle\u2014a vehicle that is being developed to improve safety\u2014we will then accept two deaths. And three. And four. Before we know it, there will be 16 pedestrian deaths per day at the hands of autonomous vehicle companies, which will never be held accountable\u2014as long as the person who was killed was walking. This story was originally published on March 20, 2018 and updated on March 22 with a link to the video released by Tempe police and additional information about the speed limit.\nRelated\nA daily dose of design and real estate news, intel, and eye candyIt provides ample space for every single bottle, can, and jar in my pantry, along with several of small appliances.Petrus Palm\u00e9r is selling his cozy and stylish family home nestled in the center of the city.Now you can explore the designer-architect\u2019s legacy from your couch.Designed by Frank Lloyd Wright in 1951, the house first hit the market in 2018 for $3.4 million, but it\u2019s now for sale at a discount.The home was constructed in 1890 by a local New Hampshire businessman.The home centers around a cavernous living and dining area with sweeping views out back."}
